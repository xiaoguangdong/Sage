#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ä½¿ç”¨çœŸå®å®è§‚æ•°æ®+åˆç†æ¨¡æ‹Ÿè¡Œä¸šæ•°æ®è¿›è¡Œ2024å¹´Q4å›æµ‹

åŠŸèƒ½ï¼š
1. ä½¿ç”¨çœŸå®çš„CPIã€PPIã€PMIã€æ”¶ç›Šç‡æ•°æ®
2. æ¨¡æ‹Ÿ2024å¹´Q4çš„è¡Œä¸šæ™¯æ°”åº¦å˜åŒ–ï¼ˆå‚è€ƒå®é™…å¸‚åœºæƒ…å†µï¼‰
3. åˆ†æé¢„æµ‹ç»“æœå’Œé‡è¦å‘ç°
"""

import pandas as pd
import numpy as np
import os
import sys
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.abspath(os.path.dirname(__file__))
sys.path.insert(0, project_root)

from ml_stock_forecast.models.macro_predictor import MacroPredictor


def load_realistic_data():
    """åŠ è½½çœŸå®å®è§‚æ•°æ®+æ¨¡æ‹Ÿè¡Œä¸šæ•°æ®"""
    print("=" * 80)
    print("åŠ è½½æ•°æ®ï¼ˆçœŸå®å®è§‚+åˆç†æ¨¡æ‹Ÿè¡Œä¸šï¼‰")
    print("=" * 80)
    
    data_dir = 'data/tushare/macro'
    
    # 1. åŠ è½½çœŸå®å®è§‚æ•°æ®
    print("\n1. åŠ è½½çœŸå®å®è§‚æ•°æ®...")
    
    # CPI
    cpi = pd.read_parquet(f'{data_dir}/tushare_cpi.parquet')
    cpi['date'] = pd.to_datetime(cpi['month'].astype(str), format='%Y%m')
    cpi = cpi[['date', 'nt_yoy']].rename(columns={'nt_yoy': 'cpi_yoy'})
    print(f"  CPI: {len(cpi)}æ¡è®°å½•")
    
    # PPI
    ppi = pd.read_parquet(f'{data_dir}/tushare_ppi.parquet')
    ppi['date'] = pd.to_datetime(ppi['month'].astype(str), format='%Y%m')
    ppi = ppi[['date', 'ppi_yoy']]
    print(f"  PPI: {len(ppi)}æ¡è®°å½•")
    
    # PMI
    pmi = pd.read_parquet(f'{data_dir}/tushare_pmi.parquet')
    pmi['date'] = pd.to_datetime(pmi['MONTH'].astype(str), format='%Y%m')
    pmi = pmi[['date', 'PMI010000']].rename(columns={'PMI010000': 'pmi'})
    print(f"  PMI: {len(pmi)}æ¡è®°å½•")
    
    # 10å¹´æœŸå›½å€ºæ”¶ç›Šç‡
    yield_10y = pd.read_parquet(f'{data_dir}/yield_10y.parquet')
    yield_10y['date'] = pd.to_datetime(yield_10y['trade_date'].astype(str), format='%Y%m%d')
    yield_10y = yield_10y[['date', 'yield']].rename(columns={'yield': 'yield_10y'})
    print(f"  æ”¶ç›Šç‡: {len(yield_10y)}æ¡è®°å½•")
    
    # åˆå¹¶å®è§‚æ•°æ®
    macro = cpi.merge(ppi, on='date', how='outer')
    macro = macro.merge(pmi, on='date', how='outer')
    macro = macro.merge(yield_10y, on='date', how='outer')
    
    # æ·»åŠ ç¤¾èå¢é€Ÿï¼ˆæ¨¡æ‹Ÿï¼‰
    macro['credit_growth'] = np.random.uniform(9.5, 11.5, len(macro))
    
    macro = macro.sort_values('date').reset_index(drop=True)
    print(f"  åˆå¹¶åå®è§‚æ•°æ®: {len(macro)}æ¡è®°å½•")
    
    # 2. åŠ è½½åŒ—å‘èµ„é‡‘æ•°æ®
    print("\n2. åŠ è½½åŒ—å‘èµ„é‡‘æ•°æ®...")
    
    # åŠ è½½åŒ—å‘èµ„é‡‘æ•°æ®
    northbound_flow = pd.read_parquet('data/tushare/northbound/daily_flow.parquet')
    northbound_flow['trade_date'] = pd.to_datetime(northbound_flow['trade_date'].astype(str), format='%Y%m%d')
    northbound_flow = northbound_flow.sort_values('trade_date')
    
    # è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
    northbound_flow['ggt_ss'] = pd.to_numeric(northbound_flow['ggt_ss'], errors='coerce')  # æ²ªè‚¡é€šï¼ˆä¸Šäº¤æ‰€åŒ—å‘ç´¯è®¡æŒä»“å¸‚å€¼ï¼‰
    northbound_flow['ggt_sz'] = pd.to_numeric(northbound_flow['ggt_sz'], errors='coerce')  # æ·±è‚¡é€šï¼ˆæ·±äº¤æ‰€åŒ—å‘ç´¯è®¡æŒä»“å¸‚å€¼ï¼‰
    
    # è®¡ç®—æ—¥åº¦å‡€æµå…¥é¢ï¼ˆå½“æ—¥ç´¯è®¡å¸‚å€¼ - å‰ä¸€æ—¥ç´¯è®¡å¸‚å€¼ï¼‰
    northbound_flow['north_inflow'] = northbound_flow['ggt_ss'].diff()  # æ²ªè‚¡é€šæ—¥åº¦å‡€æµå…¥
    northbound_flow['south_inflow'] = northbound_flow['ggt_sz'].diff()  # æ·±è‚¡é€šæ—¥åº¦å‡€æµå…¥
    # åŒ—å‘èµ„é‡‘æ—¥åº¦å‡€æµå…¥ = æ²ªè‚¡é€šæ—¥åº¦å‡€æµå…¥ + æ·±è‚¡é€šæ—¥åº¦å‡€æµå…¥
    northbound_flow['net_flow'] = northbound_flow['north_inflow'] + northbound_flow['south_inflow']
    
    print(f"  âš ï¸  æ³¨æ„: åŒ—å‘èµ„é‡‘æ•°æ®è¯´æ˜:")
    print(f"     - ggt_ss å’Œ ggt_sz æ˜¯ç´¯è®¡æŒä»“å¸‚å€¼ï¼ˆäº¿å…ƒï¼‰")
    print(f"     - é€šè¿‡å·®åˆ†è®¡ç®—æ—¥åº¦å‡€æµå…¥é¢ï¼ˆnet_flowï¼‰")
    print(f"     - 2024å¹´Q4å¹³å‡æ—¥åº¦å‡€æµå…¥: {northbound_flow[northbound_flow['trade_date'].between('2024-09-01', '2024-12-31')]['net_flow'].mean():.2f}äº¿å…ƒ")
    
    # æŒ‰è¡Œä¸šæ±‡æ€»åŒ—å‘èµ„é‡‘æµå‘ï¼ˆéœ€è¦æ ¹æ®ä»£ç æ˜ å°„åˆ°è¡Œä¸šï¼Œè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
    # åˆ›å»ºæ¨¡æ‹Ÿçš„è¡Œä¸šåŒ—å‘èµ„é‡‘æ•°æ®
    industries = [
        'éé“¶é‡‘è', 'é“¶è¡Œ', 'æˆ¿åœ°äº§', 'å»ºç­‘è£…é¥°', 'å»ºç­‘ææ–™',
        'ç”µå­', 'è®¡ç®—æœº', 'é€šä¿¡', 'ä¼ åª’',
        'æ±½è½¦', 'ç”µåŠ›è®¾å¤‡', 'å®¶ç”¨ç”µå™¨',
        'é£Ÿå“é¥®æ–™', 'å†œæ—ç‰§æ¸”', 'å•†è´¸é›¶å”®', 'ç¤¾ä¼šæœåŠ¡',
        'åŒ»è¯ç”Ÿç‰©', 'åŸºç¡€åŒ–å·¥', 'æœ‰è‰²é‡‘å±', 'é’¢é“', 'ç…¤ç‚­',
        'çŸ³æ²¹çŸ³åŒ–', 'äº¤é€šè¿è¾“', 'å…¬ç”¨äº‹ä¸š'
    ]
    
    dates = macro['date'].unique()
    northbound_industry = []
    
    for industry in industries:
        for date in dates:
            # æ¨¡æ‹Ÿè¡Œä¸šåŒ—å‘èµ„é‡‘æ•°æ®
            base_flow = np.random.uniform(-50000, 150000)
            if date >= pd.Timestamp('2024-09-01'):
                # æ”¿ç­–å—ç›Šæ¿å—åŒ—å‘èµ„é‡‘æµå…¥æ›´å¤š
                if industry in ['éé“¶é‡‘è', 'é“¶è¡Œ', 'æˆ¿åœ°äº§']:
                    base_flow = np.random.uniform(50000, 200000)
                elif industry in ['ç”µå­', 'è®¡ç®—æœº', 'é€šä¿¡']:
                    base_flow = np.random.uniform(30000, 180000)
                elif industry in ['æ±½è½¦', 'ç”µåŠ›è®¾å¤‡']:
                    base_flow = np.random.uniform(20000, 150000)
            
            northbound_industry.append({
                'industry_name': industry,
                'trade_date': date,
                'north_money': max(0, base_flow),
                'south_money': max(0, -base_flow),
                'net_flow': base_flow,  # æ·»åŠ å‡€æµå…¥å­—æ®µ
                'northbound_signal': 1 if base_flow > 50000 else 0,
                'industry_ratio': np.random.uniform(0.01, 0.08)
            })
    
    northbound_industry_df = pd.DataFrame(northbound_industry)
    northbound_industry_df = northbound_industry_df.sort_values(['industry_name', 'trade_date']).reset_index(drop=True)
    print(f"  åŒ—å‘èµ„é‡‘è¡Œä¸šæ•°æ®: {len(northbound_industry_df)}æ¡è®°å½•")
    
    # 3. åŠ è½½ç”³ä¸‡è¡Œä¸šæ•°æ®ï¼ˆåŸºäºNBSæ•°æ®ï¼‰
    print("\n3. åŠ è½½ç”³ä¸‡è¡Œä¸šæ•°æ®ï¼ˆåŸºäºNBSæ•°æ®ï¼‰...")

    # åŠ è½½ç”³ä¸‡-NBSæ˜ å°„åçš„è¡Œä¸šæ•°æ®
    nbs_result = load_nbs_industry_data(start_date='2020-01-01', end_date='2026-12-31')

    # æå–PPIå’ŒFAIæ•°æ®
    sw_ppi = nbs_result['industry_ppi']
    sw_fai = nbs_result['industry_fai']

    # åˆå¹¶PPIå’ŒFAIæ•°æ®
    industry_df = sw_ppi.merge(sw_fai, on=['sw_industry', 'date'], how='outer')

    # åªä¿ç•™éœ€è¦çš„æ—¥æœŸ
    industry_df = industry_df[industry_df['date'].isin(dates)]

    # é‡å‘½ååˆ—ä»¥åŒ¹é…æ¨¡å‹æœŸæœ›çš„å­—æ®µå
    industry_df = industry_df.rename(columns={'ppi_yoy': 'sw_ppi_yoy'})

    # æ·»åŠ æ¨¡æ‹Ÿçš„ä¼°å€¼å’ŒæµåŠ¨æ€§æ•°æ®ï¼ˆå› ä¸ºæ²¡æœ‰çœŸå®çš„ä¼°å€¼æ•°æ®ï¼‰
    # è¿™äº›æ•°æ®ä»ç„¶ä½¿ç”¨æ¨¡æ‹Ÿï¼Œå› ä¸ºéœ€è¦ä»è‚¡ç¥¨å¸‚åœºè·å–
    industry_df['pb_percentile'] = np.random.uniform(20, 80, len(industry_df))
    industry_df['pe_percentile'] = industry_df['pb_percentile'] + np.random.uniform(-10, 10, len(industry_df))
    industry_df['turnover_rate'] = np.random.uniform(0.02, 0.12, len(industry_df))
    industry_df['rps_120'] = np.random.uniform(40, 80, len(industry_df))
    industry_df['inventory_yoy'] = np.random.uniform(5, 15, len(industry_df))
    industry_df['rev_yoy'] = np.random.uniform(0, 10, len(industry_df))

    industry_df = industry_df.sort_values(['sw_industry', 'date']).reset_index(drop=True)
    print(f"  è¡Œä¸šæ•°æ®: {len(industry_df)}æ¡è®°å½•, {len(industry_df['sw_industry'].unique())}ä¸ªè¡Œä¸š")
    print(f"  âš ï¸  è¯´æ˜: PPIå’ŒFAIæ•°æ®æ¥è‡ªNBSçœŸå®æ•°æ®")
    print(f"  âš ï¸  ä¼°å€¼å’ŒæµåŠ¨æ€§æ•°æ®ä½¿ç”¨æ¨¡æ‹Ÿï¼ˆéœ€è¦ä»è‚¡ç¥¨å¸‚åœºè·å–ï¼‰")

    return macro, industry_df, northbound_industry_df


def load_nbs_industry_data(start_date='2020-01-01', end_date='2026-12-31'):
    """
    åŠ è½½NBSæ•°æ®å¹¶æ˜ å°„åˆ°ç”³ä¸‡è¡Œä¸š

    Args:
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ

    Returns:
        dict: åŒ…å«ç”³ä¸‡è¡Œä¸šæ•°æ®çš„å­—å…¸
    """
    import yaml

    data_dir = 'data/tushare/macro'

    # 1. è¯»å–NBS PPIæ•°æ®
    ppi_data = pd.read_csv(f'{data_dir}/nbs_ppi_industry_2020.csv')
    ppi_data['date'] = pd.to_datetime(ppi_data['date'].astype(str), format='%Y-%m-%d')

    # æ¸…ç†è¡Œä¸šåç§°ï¼ˆç§»é™¤åç¼€ï¼‰
    ppi_data['industry_clean'] = ppi_data['industry'].str.replace('å·¥ä¸šç”Ÿäº§è€…å‡ºå‚ä»·æ ¼æŒ‡æ•°(ä¸Šæœˆ=100)', '')

    # 2. è¯»å–NBS FAIæ•°æ®
    fai_data = pd.read_csv(f'{data_dir}/nbs_fai_industry_2020.csv')
    fai_data['date'] = pd.to_datetime(fai_data['date'].astype(str), format='%Y-%m-%d')

    # æ¸…ç†è¡Œä¸šåç§°
    fai_data['industry_clean'] = fai_data['industry'].str.replace('å›ºå®šèµ„äº§æŠ•èµ„é¢ç´¯è®¡åŒæ¯”å¢é•¿ç‡(%)', '')

    # 3. è¯»å–æ˜ å°„é…ç½®
    config_path = 'config/sw_nbs_mapping.yaml'
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    sw_to_nbs = config['sw_to_nbs']

    # 4. å°†NBSæ•°æ®æ˜ å°„åˆ°ç”³ä¸‡è¡Œä¸š
    sw_industries = list(sw_to_nbs.keys())
    dates = pd.date_range(start_date, end_date, freq='ME')

    # åˆ›å»ºç”³ä¸‡è¡Œä¸šPPIæ•°æ®
    industry_ppi = []
    for sw_industry in sw_industries:
        nbs_mappings = sw_to_nbs[sw_industry]

        for date in dates:
            # è®¡ç®—è¯¥ç”³ä¸‡è¡Œä¸šåœ¨è¯¥æ—¥æœŸçš„PPIï¼ˆåŸºäºæƒé‡èšåˆï¼‰
            total_weight = 0
            weighted_ppi = 0

            for mapping in nbs_mappings:
                nbs_industry = mapping['nbs_industry']
                weight = mapping['weight']

                # åœ¨PPIæ•°æ®ä¸­æŸ¥æ‰¾è¯¥NBSè¡Œä¸šçš„æ•°æ®
                ppi_record = ppi_data[
                    (ppi_data['industry_clean'].str.contains(nbs_industry, na=False)) &
                    (ppi_data['date'].dt.year == date.year) &
                    (ppi_data['date'].dt.month == date.month)
                ]

                if len(ppi_record) > 0:
                    # ä½¿ç”¨ç¯æ¯”æ•°æ®
                    ppi_mom = ppi_record['ppi_mom'].mean()
                    weighted_ppi += ppi_mom * weight
                    total_weight += weight

            # è®¡ç®—åŠ æƒå¹³å‡PPI
            if total_weight > 0:
                ppi_value = weighted_ppi / total_weight
            else:
                ppi_value = 100.0  # é»˜è®¤å€¼

            industry_ppi.append({
                'sw_industry': sw_industry,
                'date': date,
                'ppi_mom': ppi_value
            })

    industry_ppi_df = pd.DataFrame(industry_ppi)

    # 5. å°†FAIæ•°æ®æ˜ å°„åˆ°ç”³ä¸‡è¡Œä¸š
    industry_fai = []
    for sw_industry in sw_industries:
        nbs_mappings = sw_to_nbs[sw_industry]

        for date in dates:
            # è®¡ç®—è¯¥ç”³ä¸‡è¡Œä¸šåœ¨è¯¥æ—¥æœŸçš„FAIï¼ˆåŸºäºæƒé‡èšåˆï¼‰
            total_weight = 0
            weighted_fai = 0

            for mapping in nbs_mappings:
                nbs_industry = mapping['nbs_industry']
                weight = mapping['weight']

                # åœ¨FAIæ•°æ®ä¸­æŸ¥æ‰¾è¯¥NBSè¡Œä¸šçš„æ•°æ®
                fai_record = fai_data[
                    (fai_data['industry_clean'].str.contains(nbs_industry, na=False)) &
                    (fai_data['date'].dt.year == date.year) &
                    (fai_data['date'].dt.month == date.month)
                ]

                if len(fai_record) > 0:
                    fai_value = fai_record['fai_yoy'].mean()
                    weighted_fai += fai_value * weight
                    total_weight += weight

            # è®¡ç®—åŠ æƒå¹³å‡FAI
            if total_weight > 0:
                fai_value = weighted_fai / total_weight
            else:
                fai_value = 0.0  # é»˜è®¤å€¼

            industry_fai.append({
                'sw_industry': sw_industry,
                'date': date,
                'fai_yoy': fai_value
            })

    industry_fai_df = pd.DataFrame(industry_fai)

    # 6. è®¡ç®—PPIåŒæ¯”æ•°æ®
    industry_ppi_df = industry_ppi_df.sort_values(['sw_industry', 'date'])

    for sw_industry in sw_industries:
        sw_data = industry_ppi_df[industry_ppi_df['sw_industry'] == sw_industry].copy()

        # è®¡ç®—ç´¯è®¡æŒ‡æ•°
        sw_data['cumulative_ppi'] = 100.0
        for i in range(1, len(sw_data)):
            sw_data.iloc[i, sw_data.columns.get_loc('cumulative_ppi')] = (
                sw_data.iloc[i-1]['cumulative_ppi'] * sw_data.iloc[i]['ppi_mom'] / 100
            )

        # è®¡ç®—åŒæ¯”ï¼ˆä¸å»å¹´åŒæœŸç›¸æ¯”ï¼‰
        sw_data['ppi_yoy'] = 0.0
        for i in range(12, len(sw_data)):
            sw_data.iloc[i, sw_data.columns.get_loc('ppi_yoy')] = (
                (sw_data.iloc[i]['cumulative_ppi'] / sw_data.iloc[i-12]['cumulative_ppi'] - 1) * 100
            )

        # æ›´æ–°æ•°æ®
        industry_ppi_df.loc[industry_ppi_df['sw_industry'] == sw_industry, 'cumulative_ppi'] = sw_data['cumulative_ppi'].values
        industry_ppi_df.loc[industry_ppi_df['sw_industry'] == sw_industry, 'ppi_yoy'] = sw_data['ppi_yoy'].values

    # 7. è¿”å›ç»“æœ
    result = {
        'industry_ppi': industry_ppi_df,
        'industry_fai': industry_fai_df,
        'sw_industries': sw_industries
    }

    return result


def run_backtest():
    """è¿è¡Œå›æµ‹"""
    print("\n" + "=" * 80)
    print("2024å¹´Q4å›æµ‹åˆ†æï¼ˆçœŸå®å®è§‚+åˆç†æ¨¡æ‹Ÿï¼‰")
    print("=" * 80)
    
    # 1. åŠ è½½æ•°æ®
    macro, industry, northbound = load_realistic_data()
    
    # 2. åˆå§‹åŒ–é¢„æµ‹æ¨¡å‹
    print("\n" + "=" * 80)
    print("åˆå§‹åŒ–é¢„æµ‹æ¨¡å‹")
    print("=" * 80)
    
    predictor = MacroPredictor()
    print("é¢„æµ‹æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    # 3. å›æµ‹2024-09åˆ°2024-12
    print("\n" + "=" * 80)
    print("å›æµ‹: 2024-09-01 ~ 2024-12-31")
    print("=" * 80)
    
    backtest_dates = pd.date_range('2024-09-01', '2024-12-31', freq='D')
    print(f"å›æµ‹å¤©æ•°: {len(backtest_dates)}")
    
    results = []
    for i, date in enumerate(backtest_dates, 1):
        if i % 10 == 0:
            print(f"è¿›åº¦: {i}/{len(backtest_dates)}")
        
        result = predictor.predict(
            date=date.strftime('%Y-%m-%d'),
            macro_data=macro,
            industry_data=industry,
            northbound_data=northbound  # ä¼ å…¥åŒ—å‘èµ„é‡‘æ•°æ®
        )
        
        # è®°å½•ç»“æœ
        record = {
            'date': date,
            'systemic_scenario': result['systemic_scenario'],
            'risk_level': result['risk_level'],
            'opportunity_count': len(result['opportunity_industries'])
        }
        
        # è®°å½•TOP 5è¡Œä¸š
        for j in range(5):
            if j < len(result['opportunity_industries']):
                ind = result['opportunity_industries'][j]
                record[f'top{j+1}_industry'] = ind['industry']
                record[f'top{j+1}_scenario'] = ind['scenario']
                record[f'top{j+1}_score'] = ind['boom_score']
            else:
                record[f'top{j+1}_industry'] = ''
                record[f'top{j+1}_scenario'] = ''
                record[f'top{j+1}_score'] = 0
        
        results.append(record)
    
    # 4. åˆ†æç»“æœ
    print("\n" + "=" * 80)
    print("å›æµ‹ç»“æœåˆ†æ")
    print("=" * 80)
    
    results_df = pd.DataFrame(results)
    
    # åŸºæœ¬ç»Ÿè®¡
    print(f"\nåŸºæœ¬ç»Ÿè®¡:")
    print(f"  æ€»å¤©æ•°: {len(results_df)}")
    print(f"  ç³»ç»Ÿè¡°é€€å¤©æ•°: {len(results_df[results_df['systemic_scenario'] == 'SYSTEMIC RECESSION'])}")
    print(f"  æ­£å¸¸å¤©æ•°: {len(results_df[results_df['systemic_scenario'] == 'NORMAL'])}")
    
    # æœºä¼šè¡Œä¸šç»Ÿè®¡
    print(f"\næœºä¼šè¡Œä¸šç»Ÿè®¡:")
    print(f"  å¹³å‡æœºä¼šè¡Œä¸šæ•°: {results_df['opportunity_count'].mean():.2f}")
    print(f"  æœ€å¤§æœºä¼šè¡Œä¸šæ•°: {results_df['opportunity_count'].max()}")
    print(f"  æœ€å°æœºä¼šè¡Œä¸šæ•°: {results_df['opportunity_count'].min()}")
    print(f"  ä¸­ä½æ•°: {results_df['opportunity_count'].median():.2f}")
    
    # é£é™©ç­‰çº§åˆ†å¸ƒ
    print(f"\né£é™©ç­‰çº§åˆ†å¸ƒ:")
    risk_counts = results_df['risk_level'].value_counts()
    for risk, count in risk_counts.items():
        print(f"  {risk}: {count}å¤© ({count/len(results_df)*100:.1f}%)")
    
    # TOPè¡Œä¸šå‡ºç°é¢‘ç‡
    print(f"\nTOPè¡Œä¸šå‡ºç°é¢‘ç‡:")
    top_cols = [f'top{i}_industry' for i in range(1, 6)]
    all_top_industries = results_df[top_cols].values.flatten()
    all_top_industries = [x for x in all_top_industries if x != '']
    
    from collections import Counter
    industry_counts = Counter(all_top_industries)
    
    print(f"  TOP 10è¡Œä¸š:")
    for industry, count in industry_counts.most_common(10):
        print(f"    {industry}: {count}æ¬¡ ({count/len(results_df)*100:.1f}%)")
    
    # åœºæ™¯åˆ†å¸ƒ
    print(f"\nåœºæ™¯åˆ†å¸ƒ:")
    scenario_cols = [f'top{i}_scenario' for i in range(1, 6)]
    all_scenarios = results_df[scenario_cols].values.flatten()
    all_scenarios = [x for x in all_scenarios if x != '']
    
    scenario_counts = Counter(all_scenarios)
    for scenario, count in scenario_counts.items():
        print(f"  {scenario}: {count}æ¬¡ ({count/len(all_scenarios)*100:.1f}%)")
    
    # 5. å…³é”®æ—¶é—´ç‚¹åˆ†æ
    print("\n" + "=" * 80)
    print("å…³é”®æ—¶é—´ç‚¹åˆ†æ")
    print("=" * 80)
    
    # æ‰¾å‡ºæœºä¼šè¡Œä¸šæœ€å¤šçš„å‡ å¤©
    top_days = results_df.nlargest(5, 'opportunity_count')
    print(f"\næœºä¼šè¡Œä¸šæœ€å¤šçš„5å¤©:")
    for _, row in top_days.iterrows():
        print(f"  {row['date'].strftime('%Y-%m-%d')}: {row['opportunity_count']}ä¸ªæœºä¼šè¡Œä¸š")
        for i in range(1, 6):
            if row[f'top{i}_industry']:
                print(f"    {i}. {row[f'top{i}_industry']} ({row[f'top{i}_scenario']}) - {row[f'top{i}_score']:.1f}åˆ†")
    
    # æ‰¾å‡ºTOP 1è¡Œä¸šå˜åŒ–
    print(f"\nTOP 1è¡Œä¸šå˜åŒ–è¶‹åŠ¿:")
    top1_changes = results_df[['date', 'top1_industry', 'top1_scenario', 'top1_score']].dropna()
    current_top1 = None
    changes = []
    
    for _, row in top1_changes.iterrows():
        if row['top1_industry'] != current_top1:
            if current_top1 is not None:
                changes.append((row['date'], current_top1, row['top1_industry']))
            current_top1 = row['top1_industry']
    
    if changes:
        print(f"  å‘ç°{len(changes)}æ¬¡TOP 1è¡Œä¸šåˆ‡æ¢:")
        for date, old, new in changes:
            print(f"    {date.strftime('%Y-%m-%d')}: {old} â†’ {new}")
    
    # 6. é‡è¦å‘ç°
    print("\n" + "=" * 80)
    print("é‡è¦å‘ç°")
    print("=" * 80)
    
    discoveries = []
    
    # å‘ç°1ï¼šç³»ç»Ÿé£é™©
    recession_days = len(results_df[results_df['systemic_scenario'] == 'SYSTEMIC RECESSION'])
    if recession_days > 0:
        discoveries.append(f"âš ï¸  å‘ç°{recession_days}å¤©ç³»ç»Ÿé£é™©ä¿¡å·ï¼Œå æ¯”{recession_days/len(results_df)*100:.1f}%")
    else:
        discoveries.append(f"âœ… æ— ç³»ç»Ÿé£é™©ä¿¡å·ï¼Œå¸‚åœºç¯å¢ƒç›¸å¯¹ç¨³å®š")
    
    # å‘ç°2ï¼šä¸»å¯¼è¡Œä¸š
    if industry_counts:
        top_industry, top_count = industry_counts.most_common(1)[0]
        discoveries.append(f"ğŸ“Š {top_industry}æ˜¯ä¸»å¯¼è¡Œä¸šï¼Œå‡ºç°{top_count}æ¬¡({top_count/len(results_df)*100:.1f}%)")
    
    # å‘ç°3ï¼šå¤è‹ä¿¡å·
    recovery_count = scenario_counts.get('RECOVERY', 0) + scenario_counts.get('RECOVERY (STRONG)', 0)
    if recovery_count > 0:
        discoveries.append(f"ğŸ“ˆ å‘ç°{recovery_count}æ¬¡å¤è‹ä¿¡å·ï¼Œå æ¯”{recovery_count/len(all_scenarios)*100:.1f}%")
    
    # å‘ç°4ï¼šå¤§æ¶¨ä¿¡å·
    boom_count = scenario_counts.get('BOOM / BUBBLE', 0)
    if boom_count > 0:
        discoveries.append(f"ğŸš€ å‘ç°{boom_count}æ¬¡å¤§æ¶¨ä¿¡å·ï¼Œå æ¯”{boom_count/len(all_scenarios)*100:.1f}%")
    
    # å‘ç°5ï¼šå¹³å‡æ™¯æ°”åº¦
    score_cols = [f'top{i}_score' for i in range(1, 6)]
    all_scores = results_df[score_cols].values.flatten()
    all_scores = [x for x in all_scores if x > 0]
    if all_scores:
        avg_score = np.mean(all_scores)
        max_score = np.max(all_scores)
        discoveries.append(f"ğŸ“Š å¹³å‡æ™¯æ°”åº¦è¯„åˆ†: {avg_score:.1f}åˆ†ï¼Œæœ€é«˜: {max_score:.1f}åˆ†")
    
    # å‘ç°6ï¼šè¡Œä¸šè½®åŠ¨
    if len(changes) > 5:
        discoveries.append(f"ğŸ”„ è¡Œä¸šè½®åŠ¨é¢‘ç¹ï¼Œåˆ‡æ¢{len(changes)}æ¬¡ï¼Œè¯´æ˜å¸‚åœºç»“æ„å¿«é€Ÿå˜åŒ–")
    elif len(changes) > 0:
        discoveries.append(f"ğŸ”„ è¡Œä¸šè½®åŠ¨é€‚ä¸­ï¼Œåˆ‡æ¢{len(changes)}æ¬¡")
    
    # å‘ç°7ï¼šå®è§‚ç¯å¢ƒ
    avg_pmi = macro[macro['date'] >= pd.Timestamp('2024-09-01')]['pmi'].mean()
    avg_ppi = macro[macro['date'] >= pd.Timestamp('2024-09-01')]['ppi_yoy'].mean()
    avg_cpi = macro[macro['date'] >= pd.Timestamp('2024-09-01')]['cpi_yoy'].mean()
    avg_yield = macro[macro['date'] >= pd.Timestamp('2024-09-01')]['yield_10y'].mean()
    discoveries.append(f"ğŸŒ¡ï¸  2024å¹´Q4å®è§‚ç¯å¢ƒ: PMIå¹³å‡{avg_pmi:.1f}, PPIå¹³å‡{avg_ppi:.2f}%, CPIå¹³å‡{avg_cpi:.2f}%, 10Yå›½å€º{avg_yield:.2f}%")
    
    # å‘ç°8ï¼šåŒ—å‘èµ„é‡‘
    avg_net_flow = northbound[northbound['trade_date'] >= pd.Timestamp('2024-09-01')]['net_flow'].mean()
    discoveries.append(f"ğŸ’° åŒ—å‘èµ„é‡‘å¹³å‡å‡€æµå…¥: {avg_net_flow/10000:.1f}äº¿å…ƒ")
    
    for discovery in discoveries:
        print(f"  {discovery}")
    
    # ä¿å­˜ç»“æœ
    output_file = 'backtest_2024_q4_realistic_results.csv'
    results_df.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"\nè¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    print("\n" + "=" * 80)
    print("å›æµ‹å®Œæˆ")
    print("=" * 80)


if __name__ == '__main__':
    run_backtest()
