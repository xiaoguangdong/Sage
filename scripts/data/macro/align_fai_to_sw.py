#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
å›ºå®šèµ„äº§æŠ•èµ„æ•°æ®å¯¹é½åˆ°ç”³ä¸‡è¡Œä¸š

åŸºäºNBSæ•°æ®åˆ†æç»“æœï¼Œå°†74ä¸ªæŠ•èµ„é¢†åŸŸå¯¹é½åˆ°ç”³ä¸‡31ä¸ªä¸€çº§è¡Œä¸š
"""

import pandas as pd
import json
import yaml
import os
from typing import Dict, List

from scripts.data.macro.paths import MACRO_DIR

class FAIToSWAligner:
    """å›ºå®šèµ„äº§æŠ•èµ„æ•°æ®å¯¹é½å™¨"""
    
    def __init__(self, data_dir: str = None):
        """
        åˆå§‹åŒ–å¯¹é½å™¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•
        """
        self.data_dir = data_dir or str(MACRO_DIR)
        
        # åŠ è½½ç”³ä¸‡è¡Œä¸šæ˜ å°„é…ç½®
        mapping_file = 'config/sw_nbs_mapping.yaml'
        with open(mapping_file, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        self.sw_to_nbs = config['sw_to_nbs']
        
        # åˆ›å»ºNBSè¡Œä¸šåç§°åˆ°ç”³ä¸‡è¡Œä¸šçš„åå‘æ˜ å°„
        self.nbs_to_sw = {}
        for sw_industry, nbs_list in self.sw_to_nbs.items():
            for nbs_item in nbs_list:
                nbs_name = nbs_item['nbs_industry']
                if nbs_name not in self.nbs_to_sw:
                    self.nbs_to_sw[nbs_name] = []
                self.nbs_to_sw[nbs_name].append({
                    'sw_industry': sw_industry,
                    'weight': nbs_item['weight']
                })
    
    def parse_fai_json(self, json_file: str) -> pd.DataFrame:
        """è§£æå›ºå®šèµ„äº§æŠ•èµ„JSONæ•°æ®"""
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if 'returndata' not in data:
            return pd.DataFrame()
        
        returndata = data['returndata']
        
        # è·å–ç»´åº¦èŠ‚ç‚¹
        zb_nodes = []
        sj_nodes = []
        
        for node in returndata.get('wdnodes', []):
            if node['wdcode'] == 'zb':
                zb_nodes = node['nodes']
            elif node['wdcode'] == 'sj':
                sj_nodes = node['nodes']
        
        # åˆ›å»ºæ˜ å°„
        zb_code_to_name = {node['code']: node['name'] for node in zb_nodes}
        sj_code_to_name = {node['code']: node['name'] for node in sj_nodes}
        
        # è§£ææ•°æ®èŠ‚ç‚¹
        records = []
        for datanode in returndata.get('datanodes', []):
            if not datanode['data']['hasdata']:
                continue
            
            wds = datanode['wds']
            zb_code = None
            sj_code = None
            
            for wd in wds:
                if wd['wdcode'] == 'zb':
                    zb_code = wd['valuecode']
                elif wd['wdcode'] == 'sj':
                    sj_code = wd['valuecode']
            
            if zb_code and sj_code:
                value = datanode['data']['data']
                records.append({
                    'fai_code': zb_code,
                    'fai_name': zb_code_to_name.get(zb_code, ''),
                    'time_code': sj_code,
                    'time_name': sj_code_to_name.get(sj_code, ''),
                    'fai_yoy': value
                })
        
        df = pd.DataFrame(records)
        
        # è½¬æ¢æ—¶é—´
        if len(df) > 0:
            df['date'] = pd.to_datetime(df['time_code'].astype(str), format='%Y%m')
        
        return df
    
    def match_fai_to_nbs(self, fai_name: str) -> List[Dict]:
        """
        å°†æŠ•èµ„é¢†åŸŸåç§°åŒ¹é…åˆ°NBSè¡Œä¸š
        
        Args:
            fai_name: æŠ•èµ„é¢†åŸŸåç§°
        
        Returns:
            List[Dict]: åŒ¹é…çš„NBSè¡Œä¸šåˆ—è¡¨
        """
        matches = []
        
        # ç›´æ¥åŒ¹é…
        if fai_name in self.nbs_to_sw:
            return self.nbs_to_sw[fai_name]
        
        # æ¨¡ç³ŠåŒ¹é…ï¼šåŒ…å«å…³é”®è¯
        fai_keywords = {
            'åŒ»è¯': 'åŒ»è¯åˆ¶é€ ä¸š',
            'åŒ–å­¦': 'åŒ–å­¦åŸæ–™å’ŒåŒ–å­¦åˆ¶å“åˆ¶é€ ä¸š',
            'åŒ–å·¥': 'åŒ–å­¦åŸæ–™å’ŒåŒ–å­¦åˆ¶å“åˆ¶é€ ä¸š',
            'é’¢é“': 'é»‘è‰²é‡‘å±å†¶ç‚¼å’Œå‹å»¶åŠ å·¥ä¸š',
            'æœ‰è‰²é‡‘å±': 'æœ‰è‰²é‡‘å±å†¶ç‚¼å’Œå‹å»¶åŠ å·¥ä¸š',
            'æ±½è½¦': 'æ±½è½¦åˆ¶é€ ä¸š',
            'è®¡ç®—æœº': 'è®¡ç®—æœºã€é€šä¿¡å’Œå…¶ä»–ç”µå­è®¾å¤‡åˆ¶é€ ä¸š',
            'é€šä¿¡': 'è®¡ç®—æœºã€é€šä¿¡å’Œå…¶ä»–ç”µå­è®¾å¤‡åˆ¶é€ ä¸š',
            'ç”µå­': 'è®¡ç®—æœºã€é€šä¿¡å’Œå…¶ä»–ç”µå­è®¾å¤‡åˆ¶é€ ä¸š',
            'ç”µæ°”': 'ç”µæ°”æœºæ¢°å’Œå™¨æåˆ¶é€ ä¸š',
            'æœºæ¢°': 'é€šç”¨è®¾å¤‡åˆ¶é€ ä¸š',
            'ä¸“ç”¨è®¾å¤‡': 'ä¸“ç”¨è®¾å¤‡åˆ¶é€ ä¸š',
            'é‡‘å±': 'é‡‘å±åˆ¶å“ä¸š',
            'çŸ³æ²¹': 'çŸ³æ²¹ã€ç…¤ç‚­åŠå…¶ä»–ç‡ƒæ–™åŠ å·¥ä¸š',
            'ç…¤ç‚­': 'çŸ³æ²¹å¼€é‡‡ä¸š',
            'ç”µåŠ›': 'ç”µåŠ›ã€çƒ­åŠ›ç”Ÿäº§å’Œä¾›åº”ä¸š',
            'å»ºç­‘': 'å»ºç­‘ä¸š',
            'æˆ¿åœ°äº§': 'æˆ¿åœ°äº§ä¸š',
            'é£Ÿå“': 'é£Ÿå“åˆ¶é€ ä¸š',
            'çººç»‡': 'çººç»‡ä¸š',
            'é€ çº¸': 'é€ çº¸å’Œçº¸åˆ¶å“ä¸š',
            'åŒ»è¯': 'åŒ»è¯åˆ¶é€ ä¸š',
            'å†œå‰¯': 'å†œå‰¯é£Ÿå“åŠ å·¥ä¸š',
            'é¥®æ–™': 'é…’ã€é¥®æ–™å’Œç²¾åˆ¶èŒ¶åˆ¶é€ ä¸š',
            'å®¶å…·': 'å®¶å…·åˆ¶é€ ä¸š',
            'å°åˆ·': 'å°åˆ·å’Œè®°å½•åª’ä»‹å¤åˆ¶ä¸š',
            'æ©¡èƒ¶': 'æ©¡èƒ¶å’Œå¡‘æ–™åˆ¶å“ä¸š',
            'å¡‘æ–™': 'æ©¡èƒ¶å’Œå¡‘æ–™åˆ¶å“ä¸š',
            'éé‡‘å±': 'éé‡‘å±çŸ¿ç‰©åˆ¶å“ä¸š',
            'åºŸå¼ƒ': 'åºŸå¼ƒèµ„æºç»¼åˆåˆ©ç”¨ä¸š',
            'è¿è¾“': 'äº¤é€šè¿è¾“ã€ä»“å‚¨å’Œé‚®æ”¿ä¸š',
            'é“è·¯': 'é“è·¯è¿è¾“ä¸š',
            'é“è·¯': 'é“è·¯è¿è¾“ä¸š',
            'èˆªç©º': 'èˆªç©ºè¿è¾“ä¸š',
            'æ°´åŠ¡': 'æ°´çš„ç”Ÿäº§å’Œä¾›åº”ä¸š',
            'ç‡ƒæ°”': 'ç‡ƒæ°”ç”Ÿäº§å’Œä¾›åº”ä¸š',
            'ç¯ä¿': 'ç”Ÿæ€ä¿æŠ¤å’Œç¯å¢ƒæ²»ç†ä¸š',
        }
        
        for keyword, nbs_name in fai_keywords.items():
            if keyword in fai_name and nbs_name in self.nbs_to_sw:
                matches.extend(self.nbs_to_sw[nbs_name])
                break
        
        return matches
    
    def align_fai_to_sw(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        å¯¹é½å›ºå®šèµ„äº§æŠ•èµ„æ•°æ®åˆ°ç”³ä¸‡è¡Œä¸š
        
        Args:
            df: åŸå§‹FAIæ•°æ®
        
        Returns:
            DataFrame: å¯¹é½åçš„æ•°æ®
        """
        if len(df) == 0:
            return pd.DataFrame()
        
        print("\n" + "=" * 80)
        print("å°†å›ºå®šèµ„äº§æŠ•èµ„æ•°æ®å¯¹é½åˆ°ç”³ä¸‡è¡Œä¸š")
        print("=" * 80)
        
        aligned_records = []
        unmatched_count = 0
        
        for idx, row in df.iterrows():
            fai_name = row['fai_name']
            fai_code = row['fai_code']
            
            # åŒ¹é…åˆ°ç”³ä¸‡è¡Œä¸š
            matches = self.match_fai_to_nbs(fai_name)
            
            if not matches:
                unmatched_count += 1
                print(f"  âš ï¸  æœªåŒ¹é…: {fai_name}")
                continue
            
            # åˆ›å»ºåŒ¹é…è®°å½•
            for match in matches:
                sw_industry = match['sw_industry']
                weight = match['weight']
                
                aligned_records.append({
                    'date': row['date'],
                    'fai_code': fai_code,
                    'fai_name': fai_name,
                    'sw_industry': sw_industry,
                    'weight': weight,
                    'fai_yoy': row['fai_yoy'],
                    'fai_yoy_weighted': row['fai_yoy'] * weight
                })
        
        aligned_df = pd.DataFrame(aligned_records)
        
        if len(aligned_df) > 0:
            print(f"\n  å¯¹é½ç»“æœ:")
            print(f"    æ€»è®°å½•æ•°: {len(df)}")
            print(f"    å¯¹é½è®°å½•æ•°: {len(aligned_df)}")
            print(f"    æœªåŒ¹é…æ•°: {unmatched_count}")
            print(f"    å¯¹é½ç‡: {(len(df) - unmatched_count) / len(df) * 100:.1f}%")
            
            # ç»Ÿè®¡æ¯ä¸ªç”³ä¸‡è¡Œä¸šçš„æ•°æ®ç‚¹
            sw_coverage = aligned_df.groupby('sw_industry').agg({
                'fai_name': 'nunique',
                'date': 'nunique'
            }).reset_index()
            sw_coverage.columns = ['sw_industry', 'fai_sources', 'data_points']
            sw_coverage = sw_coverage.sort_values('data_points', ascending=False)
            
            print(f"\n  ç”³ä¸‡è¡Œä¸šè¦†ç›–æƒ…å†µ (TOP 15):")
            for i, row in sw_coverage.head(15).iterrows():
                print(f"    {i+1}. {row['sw_industry']}: {row['fai_sources']}ä¸ªFAIæº, {row['data_points']}ä¸ªæ•°æ®ç‚¹")
        
        return aligned_df
    
    def aggregate_to_sw_level(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        èšåˆåˆ°ç”³ä¸‡è¡Œä¸šçº§åˆ«
        
        Args:
            df: å¯¹é½åçš„æ•°æ®
        
        Returns:
            DataFrame: ç”³ä¸‡è¡Œä¸šçº§åˆ«çš„FAIæ•°æ®
        """
        if len(df) == 0:
            return pd.DataFrame()
        
        # æŒ‰ç”³ä¸‡è¡Œä¸šå’Œæ—¥æœŸåˆ†ç»„ï¼Œè®¡ç®—åŠ æƒå¹³å‡
        sw_fai_df = df.groupby(['sw_industry', 'date']).agg({
            'fai_yoy_weighted': 'sum',
            'weight': 'sum'
        }).reset_index()
        
        # è®¡ç®—å®é™…å¢é•¿ç‡ï¼ˆåŠ æƒå¹³å‡ï¼‰
        sw_fai_df['fai_yoy'] = sw_fai_df['fai_yoy_weighted'] / sw_fai_df['weight']
        
        # è®¡ç®—ç¯æ¯”å¢é•¿ç‡
        sw_fai_df = sw_fai_df.sort_values(['sw_industry', 'date']).reset_index(drop=True)
        sw_fai_df['fai_mom'] = sw_fai_df.groupby('sw_industry')['fai_yoy'].pct_change() * 100
        
        # é€‰æ‹©æœ€ç»ˆåˆ—
        result_df = sw_fai_df[['sw_industry', 'date', 'fai_yoy', 'fai_mom']]
        
        print(f"\n  èšåˆç»“æœ:")
        print(f"    ç”³ä¸‡è¡Œä¸šæ•°: {result_df['sw_industry'].nunique()}")
        print(f"    æ€»è®°å½•æ•°: {len(result_df)}")
        print(f"    æ—¶é—´èŒƒå›´: {result_df['date'].min()} ~ {result_df['date'].max()}")
        
        return result_df
    
    def detect_expansion_signals(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ£€æµ‹æŠ•èµ„æ‰©å¼ ä¿¡å·
        
        Args:
            df: ç”³ä¸‡è¡Œä¸šFAIæ•°æ®
        
        Returns:
            DataFrame: åŒ…å«æ‰©å¼ ä¿¡å·çš„æ•°æ®
        """
        if len(df) == 0:
            return pd.DataFrame()
        
        print("\n" + "=" * 80)
        print("æ£€æµ‹æŠ•èµ„æ‰©å¼ ä¿¡å·")
        print("=" * 80)
        
        results = []
        
        for sw_industry in df['sw_industry'].unique():
            industry_data = df[df['sw_industry'] == sw_industry].sort_values('date').reset_index(drop=True)
            
            if len(industry_data) < 3:
                continue
            
            # æ£€æŸ¥æœ€è¿‘3ä¸ªæœˆ
            recent_3m = industry_data.tail(3)
            
            # æ£€æŸ¥æ˜¯å¦è¿ç»­3ä¸ªæœˆæ­£å¢é•¿
            if (recent_3m['fai_yoy'] > 0).all():
                signal = {
                    'sw_industry': sw_industry,
                    'latest_date': recent_3m['date'].iloc[-1],
                    'fai_yoy_trend': 'EXPANSION',
                    'fai_yoy_3m_avg': recent_3m['fai_yoy'].mean(),
                    'fai_yoy_latest': recent_3m['fai_yoy'].iloc[-1],
                    'fai_mom_trend': recent_3m['fai_mom'].iloc[-1]
                }
                results.append(signal)
                
                print(f"  ğŸš€ {sw_industry}: è¿ç»­3ä¸ªæœˆæŠ•èµ„æ‰©å¼ ")
                print(f"     å¹³å‡å¢é€Ÿ: {signal['fai_yoy_3m_avg']:.2f}%, æœ€æ–°: {signal['fai_yoy_latest']:.2f}%")
        
        return pd.DataFrame(results)
    
    def process_all(self):
        """å¤„ç†æ‰€æœ‰æ•°æ®"""
        print("=" * 80)
        print("å›ºå®šèµ„äº§æŠ•èµ„æ•°æ®å¯¹é½å¤„ç†")
        print("=" * 80)
        
        # 1. è§£æFAIæ•°æ®
        print("\n1. è§£æå›ºå®šèµ„äº§æŠ•èµ„æ•°æ®...")
        fai_file = os.path.join(self.data_dir, 'A0403_å›ºå®šèµ„äº§æŠ•èµ„.json')
        if not os.path.exists(fai_file):
            print(f"  é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ {fai_file}")
            return None
        
        fai_df = self.parse_fai_json(fai_file)
        print(f"  åŸå§‹æ•°æ®: {len(fai_df)}æ¡è®°å½•")
        print(f"  æŠ•èµ„é¢†åŸŸ: {fai_df['fai_code'].nunique()}ä¸ª")
        
        # 2. å¯¹é½åˆ°ç”³ä¸‡è¡Œä¸š
        print("\n2. å¯¹é½åˆ°ç”³ä¸‡è¡Œä¸š...")
        aligned_df = self.align_fai_to_sw(fai_df)
        
        if len(aligned_df) == 0:
            print("  é”™è¯¯: å¯¹é½å¤±è´¥")
            return None
        
        # 3. èšåˆåˆ°ç”³ä¸‡è¡Œä¸šçº§åˆ«
        print("\n3. èšåˆåˆ°ç”³ä¸‡è¡Œä¸šçº§åˆ«...")
        sw_fai_df = self.aggregate_to_sw_level(aligned_df)
        
        # 4. æ£€æµ‹æ‰©å¼ ä¿¡å·
        print("\n4. æ£€æµ‹æŠ•èµ„æ‰©å¼ ä¿¡å·...")
        expansion_signals = self.detect_expansion_signals(sw_fai_df)
        
        # 5. ä¿å­˜ç»“æœ
        print("\n5. ä¿å­˜ç»“æœ...")
        output_dir = 'data/processed'
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜å¯¹é½åçš„æ•°æ®
        aligned_file = os.path.join(output_dir, 'fai_aligned_to_sw.parquet')
        aligned_df.to_parquet(aligned_file, index=False)
        print(f"  å¯¹é½æ•°æ®å·²ä¿å­˜: {aligned_file}")
        
        # ä¿å­˜èšåˆåçš„æ•°æ®
        aggregated_file = os.path.join(output_dir, 'fai_sw_industry.parquet')
        sw_fai_df.to_parquet(aggregated_file, index=False)
        print(f"  èšåˆæ•°æ®å·²ä¿å­˜: {aggregated_file}")
        
        # ä¿å­˜æ‰©å¼ ä¿¡å·
        if len(expansion_signals) > 0:
            signals_file = os.path.join(output_dir, 'fai_expansion_signals.parquet')
            expansion_signals.to_parquet(signals_file, index=False)
            print(f"  æ‰©å¼ ä¿¡å·å·²ä¿å­˜: {signals_file}")
        
        print("\n" + "=" * 80)
        print("å¤„ç†å®Œæˆ")
        print("=" * 80)
        
        return {
            'aligned': aligned_df,
            'aggregated': sw_fai_df,
            'signals': expansion_signals
        }


def main():
    """ä¸»å‡½æ•°"""
    aligner = FAIToSWAligner()
    results = aligner.process_all()
    
    if results:
        print("\nå¤„ç†ç»“æœ:")
        print(f"  å¯¹é½è®°å½•: {len(results['aligned'])}æ¡")
        print(f"  èšåˆè®°å½•: {len(results['aggregated'])}æ¡")
        print(f"  æ‰©å¼ ä¿¡å·: {len(results['signals'])}ä¸ª")


if __name__ == '__main__':
    main()
