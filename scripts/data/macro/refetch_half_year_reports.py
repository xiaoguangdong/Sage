#!/usr/bin/env python3
"""
é‡æ–°è·å–åŠå¹´æŠ¥å’Œå¹´æŠ¥çš„ä¸»è¥ä¸šåŠ¡åˆ†éƒ¨æ•°æ®
ä½¿ç”¨åˆ†é¡µè·å–ï¼Œé¿å…10,000æ¡é™åˆ¶

æ•°æ®èŒƒå›´ï¼š2020-2026å¹´çš„åŠå¹´æŠ¥(0630)å’Œå¹´æŠ¥(1231)
"""

import tushare as ts
import pandas as pd
import time
import os
import logging

from tushare_auth import get_tushare_token
from scripts.data.macro.paths import MACRO_DIR
# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class HalfYearReportFetcher:
    """åŠå¹´æŠ¥å’Œå¹´æŠ¥æ•°æ®é‡æ–°è·å–å™¨"""

    def __init__(self, token, batch_limit=8000, api_delay=45):
        """
        åˆå§‹åŒ–è·å–å™¨

        Args:
            token: Tushare API token
            batch_limit: æ¯æ¬¡åˆ†é¡µè·å–çš„è®°å½•æ•°é™åˆ¶
            api_delay: æ¯æ¬¡APIè¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰
        """
        self.pro = ts.pro_api(token)
        self.batch_limit = batch_limit
        self.api_delay = api_delay
        self.max_retries = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
        self.retry_delay = 60  # é‡è¯•å»¶è¿Ÿ60ç§’
        self.output_dir = str(MACRO_DIR / "segments")
        os.makedirs(self.output_dir, exist_ok=True)

    def get_half_year_quarters(self, start_year=2020, end_year=2026):
        """è·å–æ‰€æœ‰åŠå¹´æŠ¥å’Œå¹´æŠ¥çš„å­£åº¦åˆ—è¡¨"""
        quarters = []
        for year in range(start_year, end_year + 1):
            # åªæ·»åŠ åŠå¹´æŠ¥(0630)å’Œå¹´æŠ¥(1231)
            quarters.extend([f"{year}0630", f"{year}1231"])
        return quarters

    def fetch_mainbz_by_period(self, period):
        """
        è·å–æŒ‡å®šæœŸé—´çš„ä¸»è¥ä¸šåŠ¡åˆ†éƒ¨æ•°æ®ï¼ˆåˆ†é¡µè·å–ï¼‰

        Args:
            period: æŠ¥å‘ŠæœŸï¼Œæ ¼å¼å¦‚ '20231231'

        Returns:
            DataFrame: ä¸»è¥ä¸šåŠ¡åˆ†éƒ¨æ•°æ®
        """
        logger.info(f"\n{'='*60}")
        logger.info(f"å¼€å§‹è·å– {period} çš„ä¸»è¥ä¸šåŠ¡åˆ†éƒ¨æ•°æ®...")
        logger.info(f"{'='*60}")

        all_data = []
        offset = 0
        batch_num = 0

        try:
            while True:
                batch_num += 1
                retry_count = 0
                
                while retry_count < self.max_retries:
                    try:
                        logger.info(f"[{period}] ç¬¬ {batch_num} æ‰¹: offset={offset}, limit={self.batch_limit} (é‡è¯• {retry_count + 1}/{self.max_retries})")

                        # è°ƒç”¨ fina_mainbz_vip æ¥å£
                        df = self.pro.fina_mainbz_vip(
                            period=period,
                            offset=offset,
                            limit=self.batch_limit
                        )

                        if df is None or df.empty:
                            logger.info(f"[{period}] ç¬¬ {batch_num} æ‰¹æ— æ•°æ®ï¼Œè·å–å®Œæˆ")
                            return pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()

                        all_data.append(df)
                        logger.info(f"[{period}] ç¬¬ {batch_num} æ‰¹è·å–æˆåŠŸï¼Œå…± {len(df)} æ¡è®°å½•")

                        # æˆåŠŸè·å–ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                        break
                        
                    except Exception as e:
                        retry_count += 1
                        error_msg = str(e)
                        
                        if "IPæ•°é‡è¶…é™" in error_msg or "è¶…è¿‡2ä¸ª" in error_msg:
                            logger.warning(f"[{period}] âš ï¸ IPé™åˆ¶è§¦å‘ï¼Œç­‰å¾…{self.retry_delay}ç§’åé‡è¯•...")
                            time.sleep(self.retry_delay)
                        else:
                            logger.warning(f"[{period}] âš ï¸ è·å–å¤±è´¥: {e}ï¼Œç­‰å¾…10ç§’åé‡è¯•...")
                            time.sleep(10)
                        
                        # å¦‚æœè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
                        if retry_count >= self.max_retries:
                            logger.error(f"[{period}] âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè·å–å¤±è´¥")
                            # ä¿å­˜å·²è·å–çš„æ•°æ®
                            if all_data:
                                result_df = pd.concat(all_data, ignore_index=True)
                                filename = os.path.join(self.output_dir, f"fina_mainbz_{period}_partial.parquet")
                                result_df.to_parquet(filename, index=False)
                                logger.warning(f"[{period}] ğŸ’¾ éƒ¨åˆ†æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
                            return pd.DataFrame()

                # å¦‚æœè¿”å›çš„æ•°æ®å°‘äºlimitï¼Œè¯´æ˜å·²ç»è·å–å®Œæ‰€æœ‰æ•°æ®
                if len(df) < self.batch_limit:
                    break

                # ç»§ç»­è·å–ä¸‹ä¸€æ‰¹
                offset += self.batch_limit
                # æ‰¹æ¬¡ä¹‹é—´å»¶è¿Ÿ
                time.sleep(self.api_delay)

            # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„æ•°æ®
            if all_data:
                result_df = pd.concat(all_data, ignore_index=True)
                logger.info(f"[{period}] âœ… æ€»å…±è·å– {len(result_df):,} æ¡è®°å½• ({batch_num} æ‰¹)")

                # ä¿å­˜æ•°æ®
                filename = os.path.join(self.output_dir, f"fina_mainbz_{period}.parquet")
                result_df.to_parquet(filename, index=False)
                logger.info(f"[{period}] ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {filename}")

                return result_df
            else:
                logger.warning(f"[{period}] âš ï¸ æ²¡æœ‰æ•°æ®")
                return pd.DataFrame()

        except Exception as e:
            logger.error(f"[{period}] âŒ è·å–å¤±è´¥: {e}")
            return pd.DataFrame()

    def fetch_all_half_year_reports(self, start_year=2020, end_year=2026):
        """
        è·å–æ‰€æœ‰åŠå¹´æŠ¥å’Œå¹´æŠ¥æ•°æ®

        Args:
            start_year: èµ·å§‹å¹´ä»½
            end_year: ç»“æŸå¹´ä»½
        """
        quarters = self.get_half_year_quarters(start_year, end_year)
        total = len(quarters)

        logger.info(f"ğŸ“Š å‡†å¤‡é‡æ–°è·å– {total} ä¸ªåŠå¹´æŠ¥å’Œå¹´æŠ¥æ•°æ®ï¼ˆ{start_year}-{end_year}ï¼‰")
        logger.info(f"â±ï¸ é¢„è®¡è€—æ—¶: çº¦ {total * 2} åˆ†é’Ÿ")

        total_records = 0

        for idx, period in enumerate(quarters, 1):
            logger.info(f"\n{'#'*60}")
            logger.info(f"è¿›åº¦: {idx}/{total} ({idx/total*100:.1f}%)")
            logger.info(f"{'#'*60}")

            # è·å–æ•°æ®
            df = self.fetch_mainbz_by_period(period)

            if not df.empty:
                total_records += len(df)

            # å­£åº¦ä¹‹é—´å»¶è¿Ÿ60ç§’ï¼ˆé¿å…IPé™åˆ¶ï¼‰
            if idx < total:
                logger.info(f"\nâ³ ç­‰å¾… 60 ç§’åè·å–ä¸‹ä¸€ä¸ªå­£åº¦...")
                time.sleep(60)

        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ‰ æ‰€æœ‰æ•°æ®è·å–å®Œæˆï¼")
        logger.info(f"ğŸ“ˆ æ€»è®¡è·å–: {total_records:,} æ¡è®°å½•")
        logger.info(f"ğŸ“ æ•°æ®ä¿å­˜åœ¨: {self.output_dir}")
        logger.info(f"{'='*60}")


def main():
    """ä¸»å‡½æ•°"""
    # Tushare token
    TUSHARE_TOKEN = get_tushare_token()

    # åˆ›å»ºè·å–å™¨ï¼ˆbatch_limit=8000, api_delay=45ï¼‰
    fetcher = HalfYearReportFetcher(token=TUSHARE_TOKEN, batch_limit=8000, api_delay=45)

    # è·å–æ‰€æœ‰åŠå¹´æŠ¥å’Œå¹´æŠ¥æ•°æ®
    fetcher.fetch_all_half_year_reports(
        start_year=2020,
        end_year=2026
    )


if __name__ == "__main__":
    main()
