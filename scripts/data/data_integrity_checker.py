#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Tushareæ•°æ®å®Œæ•´æ€§æ ¡éªŒå™¨

åŠŸèƒ½ï¼š
1. è¯»å–tushare_tasks.yamlé…ç½®
2. æ£€æŸ¥æ¯ä¸ªä»»åŠ¡çš„æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
3. å¯¹äºæ—¶é—´åºåˆ—æ•°æ®ï¼Œæ£€æŸ¥2016-2026å¹´çš„æ•°æ®å®Œæ•´æ€§
4. è¾“å‡ºç¼ºå¤±æ•°æ®æŠ¥å‘Š
"""

import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(ROOT))

from collections import defaultdict
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Set, Tuple

import pandas as pd
import yaml

from scripts.data._shared.runtime import get_log_dir, get_tushare_root


class DataIntegrityChecker:
    """æ•°æ®å®Œæ•´æ€§æ ¡éªŒå™¨"""

    def __init__(
        self,
        config_path: Path,
        data_root: Path,
        start_year: int = 2016,
        end_year: int = 2026,
        light_mode: bool = True,
    ):
        """åˆå§‹åŒ–

        Args:
            config_path: tushare_tasks.yamlé…ç½®æ–‡ä»¶è·¯å¾„
            data_root: æ•°æ®æ ¹ç›®å½•
            start_year: èµ·å§‹å¹´ä»½
            end_year: ç»“æŸå¹´ä»½
        """
        self.config_path = config_path
        self.data_root = data_root
        self.start_year = start_year
        self.end_year = end_year
        self.light_mode = light_mode

        # åŠ è½½é…ç½®
        with open(config_path, "r", encoding="utf-8") as f:
            self.config = yaml.safe_load(f)

        self.tasks = self.config.get("tasks", {})
        self.integrity_exclude: Set[str] = set(self.config.get("integrity_exclude", []) or [])

        policy = self.config.get("missing_handling", {}) or {}
        delayed_grace_days = policy.get("delayed_grace_days", 7)
        try:
            delayed_grace_days = int(delayed_grace_days)
        except Exception:
            delayed_grace_days = 7
        delayed_grace_days = max(0, delayed_grace_days)

        structural_tasks = set(policy.get("structural_missing_tasks", []) or [])
        structural_tasks.update(self.integrity_exclude)
        skip_classes = set(policy.get("skip_missing_classes", ["structural_missing"]) or ["structural_missing"])

        self.missing_policy = {
            "structural_missing_tasks": structural_tasks,
            "skip_missing_classes": skip_classes,
            "delayed_grace_days": delayed_grace_days,
            "delayed_grace_by_task": policy.get("delayed_grace_by_task", {}) or {},
        }

    def _find_actual_file(self, expected_path: Path) -> Optional[Path]:
        """æŸ¥æ‰¾å®é™…çš„æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒå¤šç§å‘½åæ¨¡å¼ï¼‰

        Args:
            expected_path: é…ç½®ä¸­æœŸæœ›çš„è·¯å¾„

        Returns:
            å®é™…æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›None
        """
        # 1. ç›´æ¥æ£€æŸ¥æœŸæœ›è·¯å¾„
        if expected_path.exists():
            return expected_path

        # 2. æ£€æŸ¥ _all åç¼€ç‰ˆæœ¬
        if expected_path.suffix == ".parquet":
            all_version = expected_path.parent / f"{expected_path.stem}_all.parquet"
            if all_version.exists():
                return all_version

        # 3. æ£€æŸ¥åŒåç›®å½•ï¼ˆå¦‚ daily.parquet -> daily/ ç›®å½•ï¼‰
        stem = expected_path.stem
        same_name_dir = expected_path.parent / stem
        if same_name_dir.exists() and same_name_dir.is_dir():
            # æ£€æŸ¥ç›®å½•ä¸‹æ˜¯å¦æœ‰parquetæ–‡ä»¶
            parquet_files = list(same_name_dir.glob("*.parquet"))
            if parquet_files:
                return same_name_dir

        # 4. æ£€æŸ¥æŒ‰å¹´ä»½åˆ†å‰²çš„æ–‡ä»¶ï¼ˆå¦‚ daily/daily_2020.parquetï¼‰
        if expected_path.parent.exists():
            # æŸ¥æ‰¾åŒåç›®å½•ä¸‹çš„å¹´ä»½æ–‡ä»¶
            parent = expected_path.parent
            year_files = list(parent.glob(f"{stem}_20*.parquet"))
            if year_files:
                return parent

        # 5. æ£€æŸ¥çˆ¶ç›®å½•ä¸‹çš„åŒåæ–‡ä»¶
        parent_file = self.data_root / f"{stem}_all.parquet"
        if parent_file.exists():
            return parent_file

        # 6. æ£€æŸ¥ data/raw/tushare ç›®å½•ï¼ˆæ—§æ•°æ®ä½ç½®ï¼‰
        raw_path = self.data_root.parent / "raw" / "tushare" / expected_path.relative_to(self.data_root)
        if raw_path.exists():
            return raw_path

        return None

    def check_all(self) -> Dict[str, Dict]:
        """æ£€æŸ¥æ‰€æœ‰ä»»åŠ¡çš„æ•°æ®å®Œæ•´æ€§

        Returns:
            æ£€æŸ¥ç»“æœå­—å…¸
        """
        results = {}

        print(f"å¼€å§‹æ£€æŸ¥æ•°æ®å®Œæ•´æ€§ ({self.start_year}-{self.end_year})")
        print("=" * 80)

        for task_name, task_config in self.tasks.items():
            if task_name in self.integrity_exclude:
                print(f"\nè·³è¿‡ä»»åŠ¡: {task_name} (integrity_exclude)")
                results[task_name] = {
                    "task_name": task_name,
                    "mode": task_config.get("mode", "single"),
                    "output_path": str(self.data_root / task_config["output"]),
                    "file_exists": False,
                    "missing_data": [],
                    "status": "skipped",
                    "missing_class": "structural_missing",
                    "skip_backfill": True,
                    "skip_reason": "integrity_exclude",
                }
                continue
            print(f"\næ£€æŸ¥ä»»åŠ¡: {task_name}")
            result = self._check_task(task_name, task_config)
            result = self._attach_missing_meta(task_name, result, task_config)
            results[task_name] = result

        return results

    def _task_delayed_grace_days(self, task_name: str) -> int:
        override = (self.missing_policy.get("delayed_grace_by_task") or {}).get(task_name)
        if override is None:
            return int(self.missing_policy.get("delayed_grace_days", 7))
        try:
            return max(0, int(override))
        except Exception:
            return int(self.missing_policy.get("delayed_grace_days", 7))

    def _classify_missing(
        self, task_name: str, result: Dict[str, Any], task_config: Dict[str, Any]
    ) -> Tuple[str, bool, str]:
        status = result.get("status")
        structural_tasks: Set[str] = set(self.missing_policy.get("structural_missing_tasks") or set())
        skip_classes: Set[str] = set(self.missing_policy.get("skip_missing_classes") or set())

        if status in {"ok"}:
            return "none", False, ""
        if status == "skipped":
            return "structural_missing", True, result.get("skip_reason", "integrity_exclude")
        if task_name in structural_tasks:
            cls = "structural_missing"
            return cls, cls in skip_classes, "ä»»åŠ¡åœ¨ç»“æ„æ€§ç¼ºå¤±åå•ï¼ˆæ•°æ®æºæ— /æƒé™æœªå¼€ï¼‰"

        if status == "incomplete":
            max_date = self._parse_date(result.get("max_date"))
            if max_date is not None:
                target_end = self._target_window()[1]
                lag_days = (target_end - max_date).days
                grace_days = self._task_delayed_grace_days(task_name)
                if 0 <= lag_days <= grace_days:
                    cls = "delayed"
                    return cls, cls in skip_classes, f"æ•°æ®å‘å¸ƒå»¶è¿Ÿçª—å£å†…ï¼ˆæ»å {lag_days} å¤©ï¼Œé˜ˆå€¼ {grace_days} å¤©ï¼‰"

        cls = "error"
        return cls, cls in skip_classes, "éœ€è¦è¡¥æ•°æˆ–æ’æŸ¥æŠ“å–/å†™å…¥é”™è¯¯"

    def _attach_missing_meta(
        self, task_name: str, result: Dict[str, Any], task_config: Dict[str, Any]
    ) -> Dict[str, Any]:
        missing_class, skip_backfill, skip_reason = self._classify_missing(task_name, result, task_config)
        result["missing_class"] = missing_class
        result["skip_backfill"] = bool(skip_backfill)
        result["skip_reason"] = skip_reason
        return result

    def _read_parquet(
        self,
        path: Path,
        columns: Optional[List[str]] = None,
    ) -> pd.DataFrame:
        if not self.light_mode or not columns:
            return pd.read_parquet(path)
        try:
            return pd.read_parquet(path, columns=columns)
        except Exception:
            return pd.read_parquet(path)

    def _check_task(self, task_name: str, task_config: Dict) -> Dict:
        """æ£€æŸ¥å•ä¸ªä»»åŠ¡çš„æ•°æ®å®Œæ•´æ€§

        Args:
            task_name: ä»»åŠ¡åç§°
            task_config: ä»»åŠ¡é…ç½®

        Returns:
            æ£€æŸ¥ç»“æœ
        """
        output_path = self.data_root / task_config["output"]
        mode = task_config.get("mode", "single")

        result = {
            "task_name": task_name,
            "mode": mode,
            "output_path": str(output_path),
            "file_exists": output_path.exists(),
            "missing_data": [],
            "status": "unknown",
        }

        # 1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆæ”¯æŒå¤šç§å‘½åæ¨¡å¼ï¼‰
        actual_path = self._find_actual_file(output_path)

        if actual_path is None:
            result["status"] = "missing_file"
            print(f"  âŒ æ–‡ä»¶ä¸å­˜åœ¨: {output_path}")
            return result

        # æ›´æ–°ä¸ºå®é™…è·¯å¾„
        output_path = actual_path
        result["actual_path"] = str(actual_path)
        result["file_exists"] = True

        # 2. è¯»å–æ•°æ®ï¼ˆåˆå¹¶æ‰€æœ‰èƒ½æ‰¾åˆ°çš„æ•°æ®æºï¼‰
        try:
            dfs = []
            sources = []

            dedup_keys = task_config.get("dedup_keys") or []
            candidate_cols = [
                "trade_date",
                "ann_date",
                "end_date",
                "cal_date",
                "period",
                "f_ann_date",
            ]
            columns = list({*dedup_keys, *candidate_cols})

            if actual_path.is_dir():
                # è¯»å–ç›®å½•ä¸‹æ‰€æœ‰parquetæ–‡ä»¶
                parquet_files = list(actual_path.glob("*.parquet"))
                if not parquet_files:
                    result["status"] = "empty_directory"
                    print(f"  âŒ ç›®å½•ä¸ºç©º: {actual_path}")
                    return result
                for pf in parquet_files:
                    dfs.append(self._read_parquet(pf, columns=columns))
                sources.append(f"ç›®å½•({len(parquet_files)}ä¸ªæ–‡ä»¶)")
            else:
                dfs.append(self._read_parquet(actual_path, columns=columns))
                sources.append(actual_path.name)

            # åŒæ—¶æŸ¥æ‰¾ _all ç‰ˆæœ¬å’Œåˆ†ç‰‡ç›®å½•ï¼Œåˆå¹¶æ›´å®Œæ•´çš„æ•°æ®
            stem = Path(task_config["output"]).stem
            parent = self.data_root / Path(task_config["output"]).parent

            # æŸ¥æ‰¾ _all æ–‡ä»¶
            all_file = parent / f"{stem}_all.parquet"
            if all_file.exists() and str(all_file) != str(actual_path):
                dfs.append(self._read_parquet(all_file, columns=columns))
                sources.append(f"{all_file.name}")

            # æŸ¥æ‰¾åŒååˆ†ç‰‡ç›®å½•
            split_dir = parent / stem
            if split_dir.exists() and split_dir.is_dir() and str(split_dir) != str(actual_path):
                split_files = list(split_dir.glob("*.parquet"))
                if split_files:
                    for sf in split_files:
                        dfs.append(self._read_parquet(sf, columns=columns))
                    sources.append(f"{stem}/({len(split_files)}ä¸ªåˆ†ç‰‡)")

            # åˆå¹¶å»é‡
            df = pd.concat(dfs, ignore_index=True)
            if dedup_keys and all(k in df.columns for k in dedup_keys):
                df = df.drop_duplicates(subset=dedup_keys, keep="last")

            result["record_count"] = len(df)
            result["sources"] = sources
            print(f"  âœ… æ•°æ®æ¥æº: {', '.join(sources)}ï¼Œè®°å½•æ•°: {len(df):,}")
        except Exception as e:
            result["status"] = "read_error"
            result["error"] = str(e)
            print(f"  âŒ è¯»å–å¤±è´¥: {e}")
            return result

        # 3. æ ¹æ®æ¨¡å¼æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        if mode == "single":
            # å•æ¬¡ä¸‹è½½ä»»åŠ¡ï¼Œåªæ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
            result["status"] = "ok" if len(df) > 0 else "empty"

        elif mode == "date_range":
            # æ—¥æœŸèŒƒå›´ä»»åŠ¡ï¼Œæ£€æŸ¥æ—¶é—´è¦†ç›–
            result = self._check_date_range(df, task_config, result)

        elif mode == "year_quarters":
            # å­£åº¦ä»»åŠ¡ï¼Œæ£€æŸ¥å­£åº¦è¦†ç›–
            result = self._check_year_quarters(df, task_config, result)

        elif mode == "list":
            # åˆ—è¡¨ä»»åŠ¡ï¼Œæ£€æŸ¥æ—¶é—´è¦†ç›–ï¼ˆå¦‚æœæœ‰æ—¶é—´å­—æ®µï¼‰
            result = self._check_list_mode(df, task_config, result)

        return result

    def _check_date_range(self, df: pd.DataFrame, task_config: Dict, result: Dict) -> Dict:
        """æ£€æŸ¥æ—¥æœŸèŒƒå›´ä»»åŠ¡çš„æ•°æ®å®Œæ•´æ€§

        æ£€æŸ¥ä¸‰ä¸ªç»´åº¦ï¼š
        1. èµ·å§‹æ—¥æœŸæ˜¯å¦è¦†ç›–ç›®æ ‡èµ·å§‹
        2. ç»“æŸæ—¥æœŸæ˜¯å¦è¦†ç›–å½“å‰æ—¥æœŸï¼ˆè€Œéé¥è¿œçš„æœªæ¥ï¼‰
        3. ä¸­é—´æ˜¯å¦æœ‰å¹´ä»½ç©ºæ´
        """
        # æŸ¥æ‰¾æ—¥æœŸå­—æ®µ
        date_field = None
        for field in ["trade_date", "ann_date", "end_date"]:
            if field in df.columns:
                date_field = field
                break

        if date_field is None:
            result["status"] = "no_date_field"
            print("  âš ï¸  æœªæ‰¾åˆ°æ—¥æœŸå­—æ®µ")
            return result

        # è½¬æ¢æ—¥æœŸæ ¼å¼
        df[date_field] = pd.to_datetime(df[date_field], format="%Y%m%d", errors="coerce")

        # è·å–æ•°æ®æ—¶é—´èŒƒå›´
        min_date = df[date_field].min()
        max_date = df[date_field].max()

        result["min_date"] = min_date.strftime("%Y-%m-%d") if pd.notna(min_date) else None
        result["max_date"] = max_date.strftime("%Y-%m-%d") if pd.notna(max_date) else None

        print(f"  ğŸ“… æ—¶é—´èŒƒå›´: {result['min_date']} ~ {result['max_date']}")

        if pd.isna(min_date) or pd.isna(max_date):
            result["status"] = "invalid_dates"
            return result

        # ç”¨å½“å‰æ—¥æœŸä½œä¸ºç»“æŸç›®æ ‡ï¼ˆè€Œé end_year å¹´åº•ï¼Œæœªæ¥æ•°æ®ä¸å¯èƒ½æœ‰ï¼‰
        target_start = datetime(self.start_year, 1, 1)
        now = datetime.now()
        target_end = min(datetime(self.end_year, 12, 31), now - timedelta(days=7))

        issues = []

        # 1. æ£€æŸ¥èµ·å§‹æ—¥æœŸ
        if min_date > target_start + timedelta(days=30):
            issues.append(f"ç¼ºå°‘æ—©æœŸæ•°æ®: {target_start.year}å¹´åˆ ~ {min_date.strftime('%Y-%m-%d')}")

        # 2. æ£€æŸ¥ç»“æŸæ—¥æœŸ
        if max_date < target_end:
            issues.append(f"ç¼ºå°‘è¿‘æœŸæ•°æ®: {max_date.strftime('%Y-%m-%d')} ~ {target_end.strftime('%Y-%m-%d')}")

        # 3. æ£€æŸ¥ä¸­é—´å¹´ä»½ç©ºæ´
        years_with_data = set(df[date_field].dt.year.dropna().unique())
        expected_years = set(range(max(self.start_year, min_date.year), min(self.end_year, max_date.year) + 1))
        missing_years = sorted(expected_years - years_with_data)
        if missing_years:
            issues.append(f"ä¸­é—´å¹´ä»½ç©ºæ´: {', '.join(str(y) for y in missing_years)}")
            result["missing_years"] = missing_years

        # 4. æŒ‰å¹´ç»Ÿè®¡è®°å½•æ•°
        year_counts = df[date_field].dt.year.value_counts().sort_index()
        result["year_counts"] = {int(y): int(c) for y, c in year_counts.items()}

        if issues:
            result["status"] = "incomplete"
            result["missing_data"] = issues
            for issue in issues:
                print(f"  âš ï¸  {issue}")
        else:
            result["status"] = "ok"
            print("  âœ… æ•°æ®å®Œæ•´")

        return result

    def _parse_date(self, value: Optional[str]) -> Optional[datetime]:
        if not value:
            return None
        try:
            return datetime.strptime(value, "%Y-%m-%d")
        except Exception:
            return None

    def _format_date(self, value: datetime) -> str:
        return value.strftime("%Y%m%d")

    def _target_window(self) -> Tuple[datetime, datetime]:
        target_start = datetime(self.start_year, 1, 1)
        now = datetime.now()
        target_end = min(datetime(self.end_year, 12, 31), now - timedelta(days=7))
        return target_start, target_end

    def _infer_missing_ranges(self, result: Dict) -> List[Tuple[str, str, str]]:
        ranges: List[Tuple[str, str, str]] = []
        target_start, target_end = self._target_window()
        min_date = self._parse_date(result.get("min_date"))
        max_date = self._parse_date(result.get("max_date"))
        missing_years = result.get("missing_years") or []

        if min_date and min_date > target_start + timedelta(days=30):
            ranges.append(
                (self._format_date(target_start), self._format_date(min_date - timedelta(days=1)), "ç¼ºå°‘æ—©æœŸæ•°æ®")
            )
        if max_date and max_date < target_end - timedelta(days=1):
            ranges.append(
                (self._format_date(max_date + timedelta(days=1)), self._format_date(target_end), "ç¼ºå°‘è¿‘æœŸæ•°æ®")
            )
        for year in missing_years:
            ranges.append((f"{year}0101", f"{year}1231", f"ç¼ºå°‘å¹´ä»½ {year}"))

        if not ranges:
            ranges.append((self._format_date(target_start), self._format_date(target_end), "è¡¥é½ç¼ºå£/å…¨é‡å›è¡¥"))
        return ranges

    def build_backfill_plan(self, results: Dict[str, Dict], plan_name: Optional[str] = None) -> Dict[str, List[Dict]]:
        plan_items: List[Dict] = []
        plan_name = plan_name or f"è¡¥å……å†å²æ•°æ®_{datetime.now().strftime('%Y%m%d')}"

        for task_name, result in results.items():
            status = result.get("status")
            if status not in {"incomplete", "missing_file", "empty", "invalid_dates"}:
                continue
            if result.get("skip_backfill"):
                print(
                    f"è·³è¿‡è¡¥æ•°ä»»åŠ¡: {task_name} "
                    f"(missing_class={result.get('missing_class')}, reason={result.get('skip_reason')})"
                )
                continue
            task_config = self.tasks.get(task_name, {})
            mode = result.get("mode") or task_config.get("mode", "single")
            missing_class = result.get("missing_class", "error")

            if mode in {"date_range", "list"}:
                ranges = self._infer_missing_ranges(result)
                for start_date, end_date, reason in ranges:
                    plan_items.append(
                        {
                            "task": task_name,
                            "desc": f"{task_name} {reason}",
                            "start_date": start_date,
                            "end_date": end_date,
                            "missing_class": missing_class,
                        }
                    )
            elif mode == "year_quarters":
                missing_periods = result.get("missing_periods") or []
                years = sorted({p[:4] for p in missing_periods if len(p) >= 4})
                for year in years:
                    plan_items.append(
                        {
                            "task": task_name,
                            "desc": f"{task_name} ç¼ºå¤±å­£åº¦ {year}",
                            "start_date": f"{year}0101",
                            "end_date": f"{year}1231",
                            "missing_class": missing_class,
                        }
                    )

        return {plan_name: plan_items}

    def _check_year_quarters(self, df: pd.DataFrame, task_config: Dict, result: Dict) -> Dict:
        """æ£€æŸ¥å­£åº¦ä»»åŠ¡çš„æ•°æ®å®Œæ•´æ€§

        Args:
            df: æ•°æ®DataFrame
            task_config: ä»»åŠ¡é…ç½®
            result: å½“å‰ç»“æœ

        Returns:
            æ›´æ–°åçš„ç»“æœ
        """
        # æŸ¥æ‰¾å­£åº¦å­—æ®µ
        period_field = None
        for field in ["end_date", "period", "f_ann_date"]:
            if field in df.columns:
                period_field = field
                break

        if period_field is None:
            result["status"] = "no_period_field"
            print("  âš ï¸  æœªæ‰¾åˆ°å­£åº¦å­—æ®µ")
            return result

        # æå–å·²æœ‰çš„å­£åº¦
        df["period_str"] = df[period_field].astype(str).str[:8]
        existing_periods = set(df["period_str"].unique())

        # ç”Ÿæˆç›®æ ‡å­£åº¦åˆ—è¡¨
        start_year = task_config.get("start_year", self.start_year)
        end_year = task_config.get("end_year", self.end_year)
        quarters = task_config.get("quarters", ["0331", "0630", "0930", "1231"])

        target_periods = []
        for year in range(max(start_year, self.start_year), min(end_year, self.end_year) + 1):
            for quarter in quarters:
                target_periods.append(f"{year}{quarter}")

        # æ£€æŸ¥ç¼ºå¤±çš„å­£åº¦
        missing_periods = [p for p in target_periods if p not in existing_periods]

        result["target_periods"] = len(target_periods)
        result["existing_periods"] = len(existing_periods)
        result["missing_periods"] = missing_periods

        if missing_periods:
            result["status"] = "incomplete"
            result["missing_data"] = missing_periods
            print(f"  âš ï¸  ç¼ºå¤±å­£åº¦: {len(missing_periods)}/{len(target_periods)}")
            print(f"     {', '.join(missing_periods[:5])}{'...' if len(missing_periods) > 5 else ''}")
        else:
            result["status"] = "ok"
            print(f"  âœ… å­£åº¦å®Œæ•´: {len(target_periods)}/{len(target_periods)}")

        return result

    def _check_list_mode(self, df: pd.DataFrame, task_config: Dict, result: Dict) -> Dict:
        """æ£€æŸ¥åˆ—è¡¨æ¨¡å¼ä»»åŠ¡çš„æ•°æ®å®Œæ•´æ€§

        Args:
            df: æ•°æ®DataFrame
            task_config: ä»»åŠ¡é…ç½®
            result: å½“å‰ç»“æœ

        Returns:
            æ›´æ–°åçš„ç»“æœ
        """
        # åˆ—è¡¨æ¨¡å¼ä»»åŠ¡ï¼Œå¦‚æœæœ‰æ—¶é—´å­—æ®µåˆ™æ£€æŸ¥æ—¶é—´è¦†ç›–
        if "start_field" in task_config and "end_field" in task_config:
            return self._check_date_range(df, task_config, result)
        else:
            # æ²¡æœ‰æ—¶é—´å­—æ®µï¼Œåªæ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
            result["status"] = "ok" if len(df) > 0 else "empty"
            print("  âœ… æ•°æ®å­˜åœ¨")
            return result

    def generate_report(self, results: Dict[str, Dict]) -> str:
        """ç”Ÿæˆæ£€æŸ¥æŠ¥å‘Š

        Args:
            results: æ£€æŸ¥ç»“æœ

        Returns:
            æŠ¥å‘Šæ–‡æœ¬
        """
        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append("Tushareæ•°æ®å®Œæ•´æ€§æ£€æŸ¥æŠ¥å‘Š")
        report_lines.append(f"æ£€æŸ¥æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"ç›®æ ‡å¹´ä»½: {self.start_year}-{self.end_year}")
        report_lines.append("=" * 80)

        # ç»Ÿè®¡
        total = len(results)
        ok_count = sum(1 for r in results.values() if r["status"] == "ok")
        incomplete_count = sum(1 for r in results.values() if r["status"] == "incomplete")
        missing_count = sum(1 for r in results.values() if r["status"] == "missing_file")
        error_count = sum(1 for r in results.values() if r["status"] in ["read_error", "invalid_dates"])

        report_lines.append(f"\næ€»ä»»åŠ¡æ•°: {total}")
        report_lines.append(f"  âœ… å®Œæ•´: {ok_count}")
        report_lines.append(f"  âš ï¸  ä¸å®Œæ•´: {incomplete_count}")
        report_lines.append(f"  âŒ æ–‡ä»¶ç¼ºå¤±: {missing_count}")
        report_lines.append(f"  âŒ è¯»å–é”™è¯¯: {error_count}")

        class_counts = defaultdict(int)
        for r in results.values():
            class_counts[r.get("missing_class", "none")] += 1
        report_lines.append(
            "  åˆ†ç±»ç»Ÿè®¡: "
            f"none={class_counts['none']}, "
            f"structural_missing={class_counts['structural_missing']}, "
            f"delayed={class_counts['delayed']}, "
            f"error={class_counts['error']}"
        )

        # è¯¦ç»†ä¿¡æ¯
        report_lines.append("\n" + "=" * 80)
        report_lines.append("è¯¦ç»†ä¿¡æ¯")
        report_lines.append("=" * 80)

        # æŒ‰çŠ¶æ€åˆ†ç»„
        status_groups = defaultdict(list)
        for task_name, result in results.items():
            status_groups[result["status"]].append((task_name, result))

        # 1. æ–‡ä»¶ç¼ºå¤±
        if status_groups["missing_file"]:
            report_lines.append("\nã€æ–‡ä»¶ç¼ºå¤±ã€‘")
            for task_name, result in status_groups["missing_file"]:
                report_lines.append(f"  - {task_name}: {result['output_path']}")

        # 2. æ•°æ®ä¸å®Œæ•´
        if status_groups["incomplete"]:
            report_lines.append("\nã€æ•°æ®ä¸å®Œæ•´ã€‘")
            for task_name, result in status_groups["incomplete"]:
                report_lines.append(f"\n  {task_name}:")
                if result.get("min_date") and result.get("max_date"):
                    report_lines.append(
                        f"    å®é™…èŒƒå›´: {result['min_date']} ~ {result['max_date']}  ({result.get('record_count', '?'):,} æ¡)"
                    )
                if result.get("year_counts"):
                    years_str = ", ".join(f"{y}:{c:,}" for y, c in sorted(result["year_counts"].items()))
                    report_lines.append(f"    æŒ‰å¹´åˆ†å¸ƒ: {years_str}")
                if result.get("missing_periods"):
                    report_lines.append(f"    ç¼ºå¤±å­£åº¦: {len(result['missing_periods'])}ä¸ª")
                    report_lines.append(f"    {', '.join(result['missing_periods'][:10])}")
                    if len(result["missing_periods"]) > 10:
                        report_lines.append(f"    ... è¿˜æœ‰ {len(result['missing_periods']) - 10} ä¸ª")
                elif result.get("missing_data"):
                    for missing in result["missing_data"]:
                        report_lines.append(f"    - {missing}")
                if result.get("missing_class") and result.get("missing_class") != "none":
                    report_lines.append(
                        f"    åˆ†ç±»: {result.get('missing_class')} "
                        f"(skip_backfill={result.get('skip_backfill')}, reason={result.get('skip_reason')})"
                    )

        # 3. è¯»å–é”™è¯¯
        if status_groups["read_error"] or status_groups["invalid_dates"]:
            report_lines.append("\nã€è¯»å–é”™è¯¯ã€‘")
            for task_name, result in list(status_groups["read_error"]) + list(status_groups["invalid_dates"]):
                report_lines.append(f"  - {task_name}: {result.get('error', result['status'])}")

        # 4. å®Œæ•´æ•°æ®ï¼ˆç®€è¦åˆ—å‡ºï¼‰
        if status_groups["ok"]:
            report_lines.append(f"\nã€æ•°æ®å®Œæ•´ã€‘({len(status_groups['ok'])}ä¸ªä»»åŠ¡)")
            for task_name, result in status_groups["ok"][:5]:
                report_lines.append(f"  âœ… {task_name}")
            if len(status_groups["ok"]) > 5:
                report_lines.append(f"  ... è¿˜æœ‰ {len(status_groups['ok']) - 5} ä¸ªä»»åŠ¡")

        report_lines.append("\n" + "=" * 80)

        return "\n".join(report_lines)


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    import json

    parser = argparse.ArgumentParser(description="Tushareæ•°æ®å®Œæ•´æ€§æ ¡éªŒ")
    parser.add_argument("--config", default=str(ROOT / "config/tushare_tasks.yaml"))
    parser.add_argument("--data-root", default="")
    parser.add_argument("--start-year", type=int, default=2016)
    parser.add_argument("--end-year", type=int, default=2026)
    parser.add_argument("--report-out", default="")
    parser.add_argument("--json-out", default="")
    parser.add_argument("--plan-out", default="")
    parser.add_argument("--plan-name", default="")
    parser.add_argument("--full-scan", action="store_true", help="å…³é—­è½»é‡æ¨¡å¼ï¼Œè¯»å–å…¨é‡æ•°æ®")
    args = parser.parse_args()

    config_path = Path(args.config)
    data_root = Path(args.data_root) if args.data_root else get_tushare_root()

    checker = DataIntegrityChecker(
        config_path=config_path,
        data_root=data_root,
        start_year=args.start_year,
        end_year=args.end_year,
        light_mode=not args.full_scan,
    )

    # æ‰§è¡Œæ£€æŸ¥
    results = checker.check_all()

    # ç”ŸæˆæŠ¥å‘Š
    report = checker.generate_report(results)
    print("\n" + report)

    # ä¿å­˜æŠ¥å‘Š
    report_path = Path(args.report_out) if args.report_out else get_log_dir("data") / "data_integrity_report.txt"
    report_path.parent.mkdir(parents=True, exist_ok=True)
    report_path.write_text(report, encoding="utf-8")

    print(f"\næŠ¥å‘Šå·²ä¿å­˜: {report_path}")

    # ç”Ÿæˆè¡¥æ•°è®¡åˆ’
    plan_payload = None
    if args.plan_out:
        plan = checker.build_backfill_plan(results, plan_name=args.plan_name or None)
        plan_payload = {"download_plans": plan}
        plan_path = Path(args.plan_out)
        plan_path.parent.mkdir(parents=True, exist_ok=True)
        with plan_path.open("w", encoding="utf-8") as f:
            yaml.safe_dump(plan_payload, f, allow_unicode=True, sort_keys=False)
        print(f"è¡¥æ•°è®¡åˆ’å·²ä¿å­˜: {plan_path}")

    if args.json_out:
        payload = {"results": results, "plan": plan_payload}
        json_path = Path(args.json_out)
        json_path.parent.mkdir(parents=True, exist_ok=True)
        json_path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")


if __name__ == "__main__":
    main()
