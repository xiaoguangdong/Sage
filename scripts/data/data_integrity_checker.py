#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Tushareæ•°æ®å®Œæ•´æ€§æ ¡éªŒå™¨

åŠŸèƒ½ï¼š
1. è¯»å–tushare_tasks.yamlé…ç½®
2. æ£€æŸ¥æ¯ä¸ªä»»åŠ¡çš„æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
3. å¯¹äºæ—¶é—´åºåˆ—æ•°æ®ï¼Œæ£€æŸ¥2016-2026å¹´çš„æ•°æ®å®Œæ•´æ€§
4. è¾“å‡ºç¼ºå¤±æ•°æ®æŠ¥å‘Š
"""

import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(ROOT))

import yaml
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
from collections import defaultdict


class DataIntegrityChecker:
    """æ•°æ®å®Œæ•´æ€§æ ¡éªŒå™¨"""

    def __init__(
        self,
        config_path: Path,
        data_root: Path,
        start_year: int = 2016,
        end_year: int = 2026,
    ):
        """åˆå§‹åŒ–

        Args:
            config_path: tushare_tasks.yamlé…ç½®æ–‡ä»¶è·¯å¾„
            data_root: æ•°æ®æ ¹ç›®å½•
            start_year: èµ·å§‹å¹´ä»½
            end_year: ç»“æŸå¹´ä»½
        """
        self.config_path = config_path
        self.data_root = data_root
        self.start_year = start_year
        self.end_year = end_year

        # åŠ è½½é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)

        self.tasks = self.config.get('tasks', {})

    def _find_actual_file(self, expected_path: Path) -> Optional[Path]:
        """æŸ¥æ‰¾å®é™…çš„æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒå¤šç§å‘½åæ¨¡å¼ï¼‰

        Args:
            expected_path: é…ç½®ä¸­æœŸæœ›çš„è·¯å¾„

        Returns:
            å®é™…æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›None
        """
        # 1. ç›´æ¥æ£€æŸ¥æœŸæœ›è·¯å¾„
        if expected_path.exists():
            return expected_path

        # 2. æ£€æŸ¥ _all åç¼€ç‰ˆæœ¬
        if expected_path.suffix == '.parquet':
            all_version = expected_path.parent / f"{expected_path.stem}_all.parquet"
            if all_version.exists():
                return all_version

        # 3. æ£€æŸ¥åŒåç›®å½•ï¼ˆå¦‚ daily.parquet -> daily/ ç›®å½•ï¼‰
        stem = expected_path.stem
        same_name_dir = expected_path.parent / stem
        if same_name_dir.exists() and same_name_dir.is_dir():
            # æ£€æŸ¥ç›®å½•ä¸‹æ˜¯å¦æœ‰parquetæ–‡ä»¶
            parquet_files = list(same_name_dir.glob("*.parquet"))
            if parquet_files:
                return same_name_dir

        # 4. æ£€æŸ¥æŒ‰å¹´ä»½åˆ†å‰²çš„æ–‡ä»¶ï¼ˆå¦‚ daily/daily_2020.parquetï¼‰
        if expected_path.parent.exists():
            # æŸ¥æ‰¾åŒåç›®å½•ä¸‹çš„å¹´ä»½æ–‡ä»¶
            parent = expected_path.parent
            year_files = list(parent.glob(f"{stem}_20*.parquet"))
            if year_files:
                return parent

        # 5. æ£€æŸ¥çˆ¶ç›®å½•ä¸‹çš„åŒåæ–‡ä»¶
        parent_file = self.data_root / f"{stem}_all.parquet"
        if parent_file.exists():
            return parent_file

        # 6. æ£€æŸ¥ data/raw/tushare ç›®å½•ï¼ˆæ—§æ•°æ®ä½ç½®ï¼‰
        raw_path = self.data_root.parent / "raw" / "tushare" / expected_path.relative_to(self.data_root)
        if raw_path.exists():
            return raw_path

        return None

    def check_all(self) -> Dict[str, Dict]:
        """æ£€æŸ¥æ‰€æœ‰ä»»åŠ¡çš„æ•°æ®å®Œæ•´æ€§

        Returns:
            æ£€æŸ¥ç»“æœå­—å…¸
        """
        results = {}

        print(f"å¼€å§‹æ£€æŸ¥æ•°æ®å®Œæ•´æ€§ ({self.start_year}-{self.end_year})")
        print("=" * 80)

        for task_name, task_config in self.tasks.items():
            print(f"\næ£€æŸ¥ä»»åŠ¡: {task_name}")
            result = self._check_task(task_name, task_config)
            results[task_name] = result

        return results

    def _check_task(self, task_name: str, task_config: Dict) -> Dict:
        """æ£€æŸ¥å•ä¸ªä»»åŠ¡çš„æ•°æ®å®Œæ•´æ€§

        Args:
            task_name: ä»»åŠ¡åç§°
            task_config: ä»»åŠ¡é…ç½®

        Returns:
            æ£€æŸ¥ç»“æœ
        """
        output_path = self.data_root / task_config['output']
        mode = task_config.get('mode', 'single')

        result = {
            'task_name': task_name,
            'mode': mode,
            'output_path': str(output_path),
            'file_exists': output_path.exists(),
            'missing_data': [],
            'status': 'unknown',
        }

        # 1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆæ”¯æŒå¤šç§å‘½åæ¨¡å¼ï¼‰
        actual_path = self._find_actual_file(output_path)

        if actual_path is None:
            result['status'] = 'missing_file'
            print(f"  âŒ æ–‡ä»¶ä¸å­˜åœ¨: {output_path}")
            return result

        # æ›´æ–°ä¸ºå®é™…è·¯å¾„
        output_path = actual_path
        result['actual_path'] = str(actual_path)
        result['file_exists'] = True

        # 2. è¯»å–æ•°æ®ï¼ˆåˆå¹¶æ‰€æœ‰èƒ½æ‰¾åˆ°çš„æ•°æ®æºï¼‰
        try:
            dfs = []
            sources = []

            if actual_path.is_dir():
                # è¯»å–ç›®å½•ä¸‹æ‰€æœ‰parquetæ–‡ä»¶
                parquet_files = list(actual_path.glob("*.parquet"))
                if not parquet_files:
                    result['status'] = 'empty_directory'
                    print(f"  âŒ ç›®å½•ä¸ºç©º: {actual_path}")
                    return result
                for pf in parquet_files:
                    dfs.append(pd.read_parquet(pf))
                sources.append(f"ç›®å½•({len(parquet_files)}ä¸ªæ–‡ä»¶)")
            else:
                dfs.append(pd.read_parquet(actual_path))
                sources.append(actual_path.name)

            # åŒæ—¶æŸ¥æ‰¾ _all ç‰ˆæœ¬å’Œåˆ†ç‰‡ç›®å½•ï¼Œåˆå¹¶æ›´å®Œæ•´çš„æ•°æ®
            stem = Path(task_config['output']).stem
            parent = self.data_root / Path(task_config['output']).parent

            # æŸ¥æ‰¾ _all æ–‡ä»¶
            all_file = parent / f"{stem}_all.parquet"
            if all_file.exists() and str(all_file) != str(actual_path):
                dfs.append(pd.read_parquet(all_file))
                sources.append(f"{all_file.name}")

            # æŸ¥æ‰¾åŒååˆ†ç‰‡ç›®å½•
            split_dir = parent / stem
            if split_dir.exists() and split_dir.is_dir() and str(split_dir) != str(actual_path):
                split_files = list(split_dir.glob("*.parquet"))
                if split_files:
                    for sf in split_files:
                        dfs.append(pd.read_parquet(sf))
                    sources.append(f"{stem}/({len(split_files)}ä¸ªåˆ†ç‰‡)")

            # åˆå¹¶å»é‡
            df = pd.concat(dfs, ignore_index=True)
            dedup_keys = task_config.get('dedup_keys')
            if dedup_keys and all(k in df.columns for k in dedup_keys):
                df = df.drop_duplicates(subset=dedup_keys, keep='last')

            result['record_count'] = len(df)
            result['sources'] = sources
            print(f"  âœ… æ•°æ®æ¥æº: {', '.join(sources)}ï¼Œè®°å½•æ•°: {len(df):,}")
        except Exception as e:
            result['status'] = 'read_error'
            result['error'] = str(e)
            print(f"  âŒ è¯»å–å¤±è´¥: {e}")
            return result

        # 3. æ ¹æ®æ¨¡å¼æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        if mode == 'single':
            # å•æ¬¡ä¸‹è½½ä»»åŠ¡ï¼Œåªæ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
            result['status'] = 'ok' if len(df) > 0 else 'empty'

        elif mode == 'date_range':
            # æ—¥æœŸèŒƒå›´ä»»åŠ¡ï¼Œæ£€æŸ¥æ—¶é—´è¦†ç›–
            result = self._check_date_range(df, task_config, result)

        elif mode == 'year_quarters':
            # å­£åº¦ä»»åŠ¡ï¼Œæ£€æŸ¥å­£åº¦è¦†ç›–
            result = self._check_year_quarters(df, task_config, result)

        elif mode == 'list':
            # åˆ—è¡¨ä»»åŠ¡ï¼Œæ£€æŸ¥æ—¶é—´è¦†ç›–ï¼ˆå¦‚æœæœ‰æ—¶é—´å­—æ®µï¼‰
            result = self._check_list_mode(df, task_config, result)

        return result

    def _check_date_range(self, df: pd.DataFrame, task_config: Dict, result: Dict) -> Dict:
        """æ£€æŸ¥æ—¥æœŸèŒƒå›´ä»»åŠ¡çš„æ•°æ®å®Œæ•´æ€§

        æ£€æŸ¥ä¸‰ä¸ªç»´åº¦ï¼š
        1. èµ·å§‹æ—¥æœŸæ˜¯å¦è¦†ç›–ç›®æ ‡èµ·å§‹
        2. ç»“æŸæ—¥æœŸæ˜¯å¦è¦†ç›–å½“å‰æ—¥æœŸï¼ˆè€Œéé¥è¿œçš„æœªæ¥ï¼‰
        3. ä¸­é—´æ˜¯å¦æœ‰å¹´ä»½ç©ºæ´
        """
        # æŸ¥æ‰¾æ—¥æœŸå­—æ®µ
        date_field = None
        for field in ['trade_date', 'ann_date', 'end_date']:
            if field in df.columns:
                date_field = field
                break

        if date_field is None:
            result['status'] = 'no_date_field'
            print(f"  âš ï¸  æœªæ‰¾åˆ°æ—¥æœŸå­—æ®µ")
            return result

        # è½¬æ¢æ—¥æœŸæ ¼å¼
        df[date_field] = pd.to_datetime(df[date_field], format='%Y%m%d', errors='coerce')

        # è·å–æ•°æ®æ—¶é—´èŒƒå›´
        min_date = df[date_field].min()
        max_date = df[date_field].max()

        result['min_date'] = min_date.strftime('%Y-%m-%d') if pd.notna(min_date) else None
        result['max_date'] = max_date.strftime('%Y-%m-%d') if pd.notna(max_date) else None

        print(f"  ğŸ“… æ—¶é—´èŒƒå›´: {result['min_date']} ~ {result['max_date']}")

        if pd.isna(min_date) or pd.isna(max_date):
            result['status'] = 'invalid_dates'
            return result

        # ç”¨å½“å‰æ—¥æœŸä½œä¸ºç»“æŸç›®æ ‡ï¼ˆè€Œé end_year å¹´åº•ï¼Œæœªæ¥æ•°æ®ä¸å¯èƒ½æœ‰ï¼‰
        target_start = datetime(self.start_year, 1, 1)
        now = datetime.now()
        target_end = min(datetime(self.end_year, 12, 31), now - timedelta(days=7))

        issues = []

        # 1. æ£€æŸ¥èµ·å§‹æ—¥æœŸ
        if min_date > target_start + timedelta(days=30):
            issues.append(f"ç¼ºå°‘æ—©æœŸæ•°æ®: {target_start.year}å¹´åˆ ~ {min_date.strftime('%Y-%m-%d')}")

        # 2. æ£€æŸ¥ç»“æŸæ—¥æœŸ
        if max_date < target_end:
            issues.append(f"ç¼ºå°‘è¿‘æœŸæ•°æ®: {max_date.strftime('%Y-%m-%d')} ~ {target_end.strftime('%Y-%m-%d')}")

        # 3. æ£€æŸ¥ä¸­é—´å¹´ä»½ç©ºæ´
        years_with_data = set(df[date_field].dt.year.dropna().unique())
        expected_years = set(range(max(self.start_year, min_date.year), min(self.end_year, max_date.year) + 1))
        missing_years = sorted(expected_years - years_with_data)
        if missing_years:
            issues.append(f"ä¸­é—´å¹´ä»½ç©ºæ´: {', '.join(str(y) for y in missing_years)}")

        # 4. æŒ‰å¹´ç»Ÿè®¡è®°å½•æ•°
        year_counts = df[date_field].dt.year.value_counts().sort_index()
        result['year_counts'] = {int(y): int(c) for y, c in year_counts.items()}

        if issues:
            result['status'] = 'incomplete'
            result['missing_data'] = issues
            for issue in issues:
                print(f"  âš ï¸  {issue}")
        else:
            result['status'] = 'ok'
            print(f"  âœ… æ•°æ®å®Œæ•´")

        return result

    def _check_year_quarters(self, df: pd.DataFrame, task_config: Dict, result: Dict) -> Dict:
        """æ£€æŸ¥å­£åº¦ä»»åŠ¡çš„æ•°æ®å®Œæ•´æ€§

        Args:
            df: æ•°æ®DataFrame
            task_config: ä»»åŠ¡é…ç½®
            result: å½“å‰ç»“æœ

        Returns:
            æ›´æ–°åçš„ç»“æœ
        """
        # æŸ¥æ‰¾å­£åº¦å­—æ®µ
        period_field = None
        for field in ['end_date', 'period', 'f_ann_date']:
            if field in df.columns:
                period_field = field
                break

        if period_field is None:
            result['status'] = 'no_period_field'
            print(f"  âš ï¸  æœªæ‰¾åˆ°å­£åº¦å­—æ®µ")
            return result

        # æå–å·²æœ‰çš„å­£åº¦
        df['period_str'] = df[period_field].astype(str).str[:8]
        existing_periods = set(df['period_str'].unique())

        # ç”Ÿæˆç›®æ ‡å­£åº¦åˆ—è¡¨
        start_year = task_config.get('start_year', self.start_year)
        end_year = task_config.get('end_year', self.end_year)
        quarters = task_config.get('quarters', ['0331', '0630', '0930', '1231'])

        target_periods = []
        for year in range(max(start_year, self.start_year), min(end_year, self.end_year) + 1):
            for quarter in quarters:
                target_periods.append(f"{year}{quarter}")

        # æ£€æŸ¥ç¼ºå¤±çš„å­£åº¦
        missing_periods = [p for p in target_periods if p not in existing_periods]

        result['target_periods'] = len(target_periods)
        result['existing_periods'] = len(existing_periods)
        result['missing_periods'] = missing_periods

        if missing_periods:
            result['status'] = 'incomplete'
            result['missing_data'] = missing_periods
            print(f"  âš ï¸  ç¼ºå¤±å­£åº¦: {len(missing_periods)}/{len(target_periods)}")
            print(f"     {', '.join(missing_periods[:5])}{'...' if len(missing_periods) > 5 else ''}")
        else:
            result['status'] = 'ok'
            print(f"  âœ… å­£åº¦å®Œæ•´: {len(target_periods)}/{len(target_periods)}")

        return result

    def _check_list_mode(self, df: pd.DataFrame, task_config: Dict, result: Dict) -> Dict:
        """æ£€æŸ¥åˆ—è¡¨æ¨¡å¼ä»»åŠ¡çš„æ•°æ®å®Œæ•´æ€§

        Args:
            df: æ•°æ®DataFrame
            task_config: ä»»åŠ¡é…ç½®
            result: å½“å‰ç»“æœ

        Returns:
            æ›´æ–°åçš„ç»“æœ
        """
        # åˆ—è¡¨æ¨¡å¼ä»»åŠ¡ï¼Œå¦‚æœæœ‰æ—¶é—´å­—æ®µåˆ™æ£€æŸ¥æ—¶é—´è¦†ç›–
        if 'start_field' in task_config and 'end_field' in task_config:
            return self._check_date_range(df, task_config, result)
        else:
            # æ²¡æœ‰æ—¶é—´å­—æ®µï¼Œåªæ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
            result['status'] = 'ok' if len(df) > 0 else 'empty'
            print(f"  âœ… æ•°æ®å­˜åœ¨")
            return result

    def generate_report(self, results: Dict[str, Dict]) -> str:
        """ç”Ÿæˆæ£€æŸ¥æŠ¥å‘Š

        Args:
            results: æ£€æŸ¥ç»“æœ

        Returns:
            æŠ¥å‘Šæ–‡æœ¬
        """
        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append("Tushareæ•°æ®å®Œæ•´æ€§æ£€æŸ¥æŠ¥å‘Š")
        report_lines.append(f"æ£€æŸ¥æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"ç›®æ ‡å¹´ä»½: {self.start_year}-{self.end_year}")
        report_lines.append("=" * 80)

        # ç»Ÿè®¡
        total = len(results)
        ok_count = sum(1 for r in results.values() if r['status'] == 'ok')
        incomplete_count = sum(1 for r in results.values() if r['status'] == 'incomplete')
        missing_count = sum(1 for r in results.values() if r['status'] == 'missing_file')
        error_count = sum(1 for r in results.values() if r['status'] in ['read_error', 'invalid_dates'])

        report_lines.append(f"\næ€»ä»»åŠ¡æ•°: {total}")
        report_lines.append(f"  âœ… å®Œæ•´: {ok_count}")
        report_lines.append(f"  âš ï¸  ä¸å®Œæ•´: {incomplete_count}")
        report_lines.append(f"  âŒ æ–‡ä»¶ç¼ºå¤±: {missing_count}")
        report_lines.append(f"  âŒ è¯»å–é”™è¯¯: {error_count}")

        # è¯¦ç»†ä¿¡æ¯
        report_lines.append("\n" + "=" * 80)
        report_lines.append("è¯¦ç»†ä¿¡æ¯")
        report_lines.append("=" * 80)

        # æŒ‰çŠ¶æ€åˆ†ç»„
        status_groups = defaultdict(list)
        for task_name, result in results.items():
            status_groups[result['status']].append((task_name, result))

        # 1. æ–‡ä»¶ç¼ºå¤±
        if status_groups['missing_file']:
            report_lines.append("\nã€æ–‡ä»¶ç¼ºå¤±ã€‘")
            for task_name, result in status_groups['missing_file']:
                report_lines.append(f"  - {task_name}: {result['output_path']}")

        # 2. æ•°æ®ä¸å®Œæ•´
        if status_groups['incomplete']:
            report_lines.append("\nã€æ•°æ®ä¸å®Œæ•´ã€‘")
            for task_name, result in status_groups['incomplete']:
                report_lines.append(f"\n  {task_name}:")
                if result.get('min_date') and result.get('max_date'):
                    report_lines.append(f"    å®é™…èŒƒå›´: {result['min_date']} ~ {result['max_date']}  ({result.get('record_count', '?'):,} æ¡)")
                if result.get('year_counts'):
                    years_str = ', '.join(f"{y}:{c:,}" for y, c in sorted(result['year_counts'].items()))
                    report_lines.append(f"    æŒ‰å¹´åˆ†å¸ƒ: {years_str}")
                if result.get('missing_periods'):
                    report_lines.append(f"    ç¼ºå¤±å­£åº¦: {len(result['missing_periods'])}ä¸ª")
                    report_lines.append(f"    {', '.join(result['missing_periods'][:10])}")
                    if len(result['missing_periods']) > 10:
                        report_lines.append(f"    ... è¿˜æœ‰ {len(result['missing_periods']) - 10} ä¸ª")
                elif result.get('missing_data'):
                    for missing in result['missing_data']:
                        report_lines.append(f"    - {missing}")

        # 3. è¯»å–é”™è¯¯
        if status_groups['read_error'] or status_groups['invalid_dates']:
            report_lines.append("\nã€è¯»å–é”™è¯¯ã€‘")
            for task_name, result in list(status_groups['read_error']) + list(status_groups['invalid_dates']):
                report_lines.append(f"  - {task_name}: {result.get('error', result['status'])}")

        # 4. å®Œæ•´æ•°æ®ï¼ˆç®€è¦åˆ—å‡ºï¼‰
        if status_groups['ok']:
            report_lines.append(f"\nã€æ•°æ®å®Œæ•´ã€‘({len(status_groups['ok'])}ä¸ªä»»åŠ¡)")
            for task_name, result in status_groups['ok'][:5]:
                report_lines.append(f"  âœ… {task_name}")
            if len(status_groups['ok']) > 5:
                report_lines.append(f"  ... è¿˜æœ‰ {len(status_groups['ok']) - 5} ä¸ªä»»åŠ¡")

        report_lines.append("\n" + "=" * 80)

        return "\n".join(report_lines)


def main():
    """ä¸»å‡½æ•°"""
    config_path = ROOT / "config/tushare_tasks.yaml"
    data_root = ROOT / "data/tushare"

    checker = DataIntegrityChecker(
        config_path=config_path,
        data_root=data_root,
        start_year=2016,
        end_year=2026,
    )

    # æ‰§è¡Œæ£€æŸ¥
    results = checker.check_all()

    # ç”ŸæˆæŠ¥å‘Š
    report = checker.generate_report(results)
    print("\n" + report)

    # ä¿å­˜æŠ¥å‘Š
    report_path = ROOT / "logs/data/data_integrity_report.txt"
    report_path.parent.mkdir(parents=True, exist_ok=True)
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"\næŠ¥å‘Šå·²ä¿å­˜: {report_path}")


if __name__ == "__main__":
    main()
