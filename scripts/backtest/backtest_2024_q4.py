#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ä½¿ç”¨çœŸå®æ•°æ®å›æµ‹2024å¹´Q4

åŠŸèƒ½ï¼š
1. åŠ è½½çœŸå®çš„Tushareå’ŒNBSæ•°æ®
2. å¯¹2024-09åˆ°2024-12è¿›è¡Œå›æµ‹
3. åˆ†æé¢„æµ‹ç»“æœå’Œé‡è¦å‘ç°
"""

import pandas as pd
import numpy as np
import os
import re
import sys
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.insert(0, project_root)

from sage_core.industry.macro_predictor import MacroPredictor
from scripts.data._shared.runtime import get_data_path


def load_real_data():
    """åŠ è½½çœŸå®æ•°æ®"""
    print("=" * 80)
    print("åŠ è½½çœŸå®æ•°æ®")
    print("=" * 80)
    
    data_dir = str(get_data_path("raw", "tushare", "macro"))
    
    # 1. åŠ è½½å®è§‚æ•°æ®
    print("\n1. åŠ è½½å®è§‚æ•°æ®...")
    
    # CPI
    cpi = pd.read_parquet(f'{data_dir}/tushare_cpi.parquet')
    cpi['date'] = pd.to_datetime(cpi['month'].astype(str), format='%Y%m')
    cpi = cpi[['date', 'nt_yoy']].rename(columns={'nt_yoy': 'cpi_yoy'})
    print(f"  CPI: {len(cpi)}æ¡è®°å½• ({cpi['date'].min()} ~ {cpi['date'].max()})")
    
    # PPI
    ppi = pd.read_parquet(f'{data_dir}/tushare_ppi.parquet')
    ppi['date'] = pd.to_datetime(ppi['month'].astype(str), format='%Y%m')
    ppi = ppi[['date', 'ppi_yoy']]
    print(f"  PPI: {len(ppi)}æ¡è®°å½• ({ppi['date'].min()} ~ {ppi['date'].max()})")
    
    # PMI
    pmi = pd.read_parquet(f'{data_dir}/tushare_pmi.parquet')
    pmi['date'] = pd.to_datetime(pmi['MONTH'].astype(str), format='%Y%m')
    pmi = pmi[['date', 'PMI010000']].rename(columns={'PMI010000': 'pmi'})
    print(f"  PMI: {len(pmi)}æ¡è®°å½• ({pmi['date'].min()} ~ {pmi['date'].max()})")
    
    # 10å¹´æœŸå›½å€ºæ”¶ç›Šç‡
    yield_10y = pd.read_parquet(f'{data_dir}/yield_10y.parquet')
    yield_10y['date'] = pd.to_datetime(yield_10y['trade_date'].astype(str), format='%Y%m%d')
    yield_10y = yield_10y[['date', 'yield']].rename(columns={'yield': 'yield_10y'})
    print(f"  æ”¶ç›Šç‡: {len(yield_10y)}æ¡è®°å½• ({yield_10y['date'].min()} ~ {yield_10y['date'].max()})")
    
    # ç¤¾èæ•°æ®
    credit = pd.read_parquet(f'{data_dir}/credit_data.parquet')
    credit['date'] = pd.to_datetime(credit['date'])
    if 'credit_growth' in credit.columns:
        print(f"  ç¤¾è: {len(credit)}æ¡è®°å½• ({credit['date'].min()} ~ {credit['date'].max()})")
    else:
        credit = pd.DataFrame(columns=['date', 'credit_growth'])
    
    # åˆå¹¶å®è§‚æ•°æ®
    macro = cpi.merge(ppi, on='date', how='outer')
    macro = macro.merge(pmi, on='date', how='outer')
    macro = macro.merge(yield_10y, on='date', how='outer')
    if len(credit) > 0:
        macro = macro.merge(credit[['date', 'credit_growth']], on='date', how='left')
    
    macro = macro.sort_values('date').reset_index(drop=True)
    print(f"  åˆå¹¶åå®è§‚æ•°æ®: {len(macro)}æ¡è®°å½•")
    
    # 2. åŠ è½½è¡Œä¸šæ•°æ®
    print("\n2. åŠ è½½è¡Œä¸šæ•°æ®...")
    
    # ç”³ä¸‡L1 PPI
    sw_l1 = pd.read_csv(f'{data_dir}/sw_l1_ppi_yoy_202512.csv')
    sw_l1['date'] = pd.to_datetime(sw_l1['date'])
    sw_l1 = sw_l1.rename(columns={'sw_industry': 'sw_industry', 'sw_ppi_yoy': 'sw_ppi_yoy'})
    print(f"  ç”³ä¸‡L1 PPI: {len(sw_l1)}æ¡è®°å½•")
    
    # ç”³ä¸‡L2 PPI
    sw_l2 = pd.read_csv(f'{data_dir}/sw_l2_ppi_yoy_202512.csv')
    sw_l2['date'] = pd.to_datetime(sw_l2['date'])
    sw_l2 = sw_l2.rename(columns={'sw_industry': 'sw_industry', 'sw_ppi_yoy': 'sw_ppi_yoy'})
    print(f"  ç”³ä¸‡L2 PPI: {len(sw_l2)}æ¡è®°å½•")
    
    # åˆå¹¶è¡Œä¸šæ•°æ®
    sw_industry_df = pd.concat([sw_l1, sw_l2], ignore_index=True)
    sw_industry_df = sw_industry_df.sort_values(['sw_industry', 'date']).reset_index(drop=True)
    print(f"  åˆå¹¶åè¡Œä¸šæ•°æ®: {len(sw_industry_df)}æ¡è®°å½•, {len(sw_industry_df['sw_industry'].unique())}ä¸ªè¡Œä¸š")
    
    # 3. æ·»åŠ æ¨¡æ‹Ÿçš„å¸‚åœºæ•°æ®ï¼ˆä¼°å€¼ã€æ¢æ‰‹ç‡ç­‰ï¼‰
    print("\n3. æ·»åŠ å¸‚åœºæ•°æ®...")
    
    industries_list = sw_industry_df['sw_industry'].unique()
    dates_list = macro['date'].unique()
    
    market_data = []
    for ind_name in industries_list:
        for date in dates_list:
            market_data.append({
                'sw_industry': ind_name,
                'date': date,
                'pb_percentile': np.random.uniform(20, 80),
                'pe_percentile': np.random.uniform(20, 80),
                'turnover_rate': np.random.uniform(0.02, 0.10),
                'rps_120': np.random.uniform(40, 80),
                'inventory_yoy': np.random.uniform(5, 15),
                'rev_yoy': np.random.uniform(0, 10),
                'fai_yoy': np.random.uniform(2, 12)
            })
    
    market_df = pd.DataFrame(market_data)
    
    # åˆå¹¶åˆ°è¡Œä¸šæ•°æ®
    industry_final = sw_industry_df.merge(market_df, on=['sw_industry', 'date'], how='left')
    
    # ç¡®ä¿æœ‰å¿…è¦çš„åˆ—
    required_cols = ['sw_industry', 'date', 'sw_ppi_yoy', 'fai_yoy', 
                    'pb_percentile', 'turnover_rate', 'rps_120',
                    'inventory_yoy', 'rev_yoy']
    
    for col in required_cols:
        if col not in industry_final.columns:
            if col == 'inventory_yoy':
                industry_final[col] = 0
            elif col == 'rev_yoy':
                industry_final[col] = 0
            elif col == 'fai_yoy':
                industry_final[col] = 0
            else:
                industry_final[col] = 0
    
    print(f"  æœ€ç»ˆè¡Œä¸šæ•°æ®: {len(industry_final)}æ¡è®°å½•")
    
    return macro, industry_final


def build_output_path(base_name: str) -> str:
    log_dir = os.path.join('logs', 'backtest')
    os.makedirs(log_dir, exist_ok=True)
    date_str = datetime.now().strftime('%Y%m%d')
    pattern = re.compile(rf'^{date_str}_(\\d{{3}})_{re.escape(base_name)}$')
    next_seq = 1
    for name in os.listdir(log_dir):
        match = pattern.match(name)
        if match:
            next_seq = max(next_seq, int(match.group(1)) + 1)
    return os.path.join(log_dir, f'{date_str}_{next_seq:03d}_{base_name}')


def run_backtest():
    """è¿è¡Œå›æµ‹"""
    print("\n" + "=" * 80)
    print("2024å¹´Q4å›æµ‹åˆ†æ")
    print("=" * 80)
    
    # 1. åŠ è½½æ•°æ®
    macro, industry = load_real_data()
    
    # 2. åˆå§‹åŒ–é¢„æµ‹æ¨¡å‹
    print("\n" + "=" * 80)
    print("åˆå§‹åŒ–é¢„æµ‹æ¨¡å‹")
    print("=" * 80)
    
    predictor = MacroPredictor()
    print("é¢„æµ‹æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    # 3. å›æµ‹2024-09åˆ°2024-12
    print("\n" + "=" * 80)
    print("å›æµ‹: 2024-09-01 ~ 2024-12-31")
    print("=" * 80)
    
    backtest_dates = pd.date_range('2024-09-01', '2024-12-31', freq='D')
    print(f"å›æµ‹å¤©æ•°: {len(backtest_dates)}")
    
    results = []
    for i, date in enumerate(backtest_dates, 1):
        if i % 10 == 0:
            print(f"è¿›åº¦: {i}/{len(backtest_dates)}")
        
        result = predictor.predict(
            date=date.strftime('%Y-%m-%d'),
            macro_data=macro,
            industry_data=industry,
            northbound_data=None
        )
        
        # è®°å½•ç»“æœ
        record = {
            'date': date,
            'systemic_scenario': result['systemic_scenario'],
            'risk_level': result['risk_level'],
            'opportunity_count': len(result['opportunity_industries'])
        }
        
        # è®°å½•TOP 5è¡Œä¸š
        for j in range(5):
            if j < len(result['opportunity_industries']):
                ind = result['opportunity_industries'][j]
                record[f'top{j+1}_industry'] = ind['industry']
                record[f'top{j+1}_scenario'] = ind['scenario']
                record[f'top{j+1}_score'] = ind['boom_score']
            else:
                record[f'top{j+1}_industry'] = ''
                record[f'top{j+1}_scenario'] = ''
                record[f'top{j+1}_score'] = 0
        
        results.append(record)
    
    # 4. åˆ†æç»“æœ
    print("\n" + "=" * 80)
    print("å›æµ‹ç»“æœåˆ†æ")
    print("=" * 80)
    
    results_df = pd.DataFrame(results)
    
    # åŸºæœ¬ç»Ÿè®¡
    print(f"\nåŸºæœ¬ç»Ÿè®¡:")
    print(f"  æ€»å¤©æ•°: {len(results_df)}")
    print(f"  ç³»ç»Ÿè¡°é€€å¤©æ•°: {len(results_df[results_df['systemic_scenario'] == 'SYSTEMIC RECESSION'])}")
    print(f"  æ­£å¸¸å¤©æ•°: {len(results_df[results_df['systemic_scenario'] == 'NORMAL'])}")
    
    # æœºä¼šè¡Œä¸šç»Ÿè®¡
    print(f"\næœºä¼šè¡Œä¸šç»Ÿè®¡:")
    print(f"  å¹³å‡æœºä¼šè¡Œä¸šæ•°: {results_df['opportunity_count'].mean():.2f}")
    print(f"  æœ€å¤§æœºä¼šè¡Œä¸šæ•°: {results_df['opportunity_count'].max()}")
    print(f"  æœ€å°æœºä¼šè¡Œä¸šæ•°: {results_df['opportunity_count'].min()}")
    print(f"  ä¸­ä½æ•°: {results_df['opportunity_count'].median():.2f}")
    
    # é£é™©ç­‰çº§åˆ†å¸ƒ
    print(f"\né£é™©ç­‰çº§åˆ†å¸ƒ:")
    risk_counts = results_df['risk_level'].value_counts()
    for risk, count in risk_counts.items():
        print(f"  {risk}: {count}å¤© ({count/len(results_df)*100:.1f}%)")
    
    # TOPè¡Œä¸šå‡ºç°é¢‘ç‡
    print(f"\nTOPè¡Œä¸šå‡ºç°é¢‘ç‡:")
    top_cols = [f'top{i}_industry' for i in range(1, 6)]
    all_top_industries = results_df[top_cols].values.flatten()
    all_top_industries = [x for x in all_top_industries if x != '']
    
    from collections import Counter
    industry_counts = Counter(all_top_industries)
    
    print(f"  TOP 10è¡Œä¸š:")
    for industry, count in industry_counts.most_common(10):
        print(f"    {industry}: {count}æ¬¡ ({count/len(results_df)*100:.1f}%)")
    
    # åœºæ™¯åˆ†å¸ƒ
    print(f"\nåœºæ™¯åˆ†å¸ƒ:")
    scenario_cols = [f'top{i}_scenario' for i in range(1, 6)]
    all_scenarios = results_df[scenario_cols].values.flatten()
    all_scenarios = [x for x in all_scenarios if x != '']
    
    scenario_counts = Counter(all_scenarios)
    for scenario, count in scenario_counts.items():
        print(f"  {scenario}: {count}æ¬¡ ({count/len(all_scenarios)*100:.1f}%)")
    
    # 5. å…³é”®æ—¶é—´ç‚¹åˆ†æ
    print("\n" + "=" * 80)
    print("å…³é”®æ—¶é—´ç‚¹åˆ†æ")
    print("=" * 80)
    
    # æ‰¾å‡ºæœºä¼šè¡Œä¸šæœ€å¤šçš„å‡ å¤©
    top_days = results_df.nlargest(5, 'opportunity_count')
    print(f"\næœºä¼šè¡Œä¸šæœ€å¤šçš„5å¤©:")
    for _, row in top_days.iterrows():
        print(f"  {row['date'].strftime('%Y-%m-%d')}: {row['opportunity_count']}ä¸ªæœºä¼šè¡Œä¸š")
        for i in range(1, 6):
            if row[f'top{i}_industry']:
                print(f"    {i}. {row[f'top{i}_industry']} ({row[f'top{i}_scenario']}) - {row[f'top{i}_score']:.1f}åˆ†")
    
    # æ‰¾å‡ºTOP 1è¡Œä¸šå˜åŒ–
    print(f"\nTOP 1è¡Œä¸šå˜åŒ–è¶‹åŠ¿:")
    top1_changes = results_df[['date', 'top1_industry', 'top1_scenario', 'top1_score']].dropna()
    current_top1 = None
    changes = []
    
    for _, row in top1_changes.iterrows():
        if row['top1_industry'] != current_top1:
            if current_top1 is not None:
                changes.append((row['date'], current_top1, row['top1_industry']))
            current_top1 = row['top1_industry']
    
    if changes:
        print(f"  å‘ç°{len(changes)}æ¬¡TOP 1è¡Œä¸šåˆ‡æ¢:")
        for date, old, new in changes:
            print(f"    {date.strftime('%Y-%m-%d')}: {old} â†’ {new}")
    
    # 6. é‡è¦å‘ç°
    print("\n" + "=" * 80)
    print("é‡è¦å‘ç°")
    print("=" * 80)
    
    discoveries = []
    
    # å‘ç°1ï¼šç³»ç»Ÿé£é™©
    recession_days = len(results_df[results_df['systemic_scenario'] == 'SYSTEMIC RECESSION'])
    if recession_days > 0:
        discoveries.append(f"âš ï¸  å‘ç°{recession_days}å¤©ç³»ç»Ÿé£é™©ä¿¡å·ï¼Œå æ¯”{recession_days/len(results_df)*100:.1f}%")
    
    # å‘ç°2ï¼šä¸»å¯¼è¡Œä¸š
    if industry_counts:
        top_industry, top_count = industry_counts.most_common(1)[0]
        discoveries.append(f"ğŸ“Š {top_industry}æ˜¯ä¸»å¯¼è¡Œä¸šï¼Œå‡ºç°{top_count}æ¬¡({top_count/len(results_df)*100:.1f}%)")
    
    # å‘ç°3ï¼šå¤è‹ä¿¡å·
    recovery_count = scenario_counts.get('RECOVERY', 0) + scenario_counts.get('RECOVERY (STRONG)', 0)
    if recovery_count > 0:
        discoveries.append(f"ğŸ“ˆ å‘ç°{recovery_count}æ¬¡å¤è‹ä¿¡å·ï¼Œå æ¯”{recovery_count/len(all_scenarios)*100:.1f}%")
    
    # å‘ç°4ï¼šå¤§æ¶¨ä¿¡å·
    boom_count = scenario_counts.get('BOOM / BUBBLE', 0)
    if boom_count > 0:
        discoveries.append(f"ğŸš€ å‘ç°{boom_count}æ¬¡å¤§æ¶¨ä¿¡å·ï¼Œå æ¯”{boom_count/len(all_scenarios)*100:.1f}%")
    
    # å‘ç°5ï¼šå¹³å‡æ™¯æ°”åº¦
    score_cols = [f'top{i}_score' for i in range(1, 6)]
    all_scores = results_df[score_cols].values.flatten()
    all_scores = [x for x in all_scores if x > 0]
    if all_scores:
        avg_score = np.mean(all_scores)
        discoveries.append(f"ğŸ“Š å¹³å‡æ™¯æ°”åº¦è¯„åˆ†: {avg_score:.1f}åˆ†")
    
    for discovery in discoveries:
        print(f"  {discovery}")
    
    # ä¿å­˜ç»“æœ
    output_file = build_output_path('backtest_2024_q4_results.csv')
    results_df.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"\nè¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    print("\n" + "=" * 80)
    print("å›æµ‹å®Œæˆ")
    print("=" * 80)


if __name__ == '__main__':
    run_backtest()
